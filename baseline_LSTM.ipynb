{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UgKbXYvqPjZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from ast import literal_eval\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fFEVH58Og0T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "64f66826-97b5-4f87-d063-96db8679fb23"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lmu-mandy/project-rgt/bob-branch/ted_talks_en.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df = df.loc[:, ['talk_id', 'topics', 'transcript']]\n",
        "df.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>talk_id</th>\n",
              "      <th>topics</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92</td>\n",
              "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
              "      <td>If you're here today — and I'm very happy that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66</td>\n",
              "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
              "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   talk_id  ...                                         transcript\n",
              "0        1  ...  Thank you so much, Chris. And it's truly a gre...\n",
              "1       92  ...  About 10 years ago, I took on the task to teac...\n",
              "2        7  ...  (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "3       53  ...  If you're here today — and I'm very happy that...\n",
              "4       66  ...  Good morning. How are you? (Audience) Good. It...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRHtVN-7fgNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f868e406-41b6-4342-f5c1-44e021923d85"
      },
      "source": [
        "sep_topics = df.topics.unique()\n",
        "topics = []\n",
        "\n",
        "for topic in sep_topics:\n",
        "    for i in topic.split(\",\"):\n",
        "        topics.append(i.split(\"'\")[1])\n",
        "print(topics[0:5])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alternative energy', 'cars', 'climate change', 'culture', 'environment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC2uwR-X-uUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c02c7a6-9ac3-4124-9794-3b6559f3d8fd"
      },
      "source": [
        "unique_topics = [] \n",
        "      \n",
        "# traverse for all elements \n",
        "for topic in topics: \n",
        "    # check if exists in unique_list or not \n",
        "    if topic not in unique_topics: \n",
        "            unique_topics.append(topic) \n",
        "print(unique_topics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alternative energy', 'cars', 'climate change', 'culture', 'environment', 'global issues', 'science', 'sustainability', 'technology', 'Africa', 'Asia', 'Google', 'demo', 'economics', 'health', 'statistics', 'global development', 'visualizations', 'math', 'computers', 'entertainment', 'interface design', 'media', 'music', 'performance', 'simplicity', 'software', 'MacArthur grant', 'activism', 'business', 'cities', 'green', 'inequality', 'politics', 'pollution', 'children', 'creativity', 'dance', 'education', 'parenting', 'teaching', 'architecture', 'collaboration', 'design', 'library', 'Christianity', 'God', 'atheism', 'comedy', 'religion', 'storytelling', 'humor', 'brain', 'cognitive science', 'consciousness', 'evolution', 'philosophy', 'happiness', 'leadership', 'motivation', 'philanthropy', 'TED Prize', 'film', 'peace', 'social change', 'art', 'movies', 'disease', 'ebola', 'disaster relief', 'invention', 'open-source', 'entrepreneur', 'piano', 'wunderkind', 'live music', 'violin', 'youth', 'engineering', 'industrial design', 'DNA', 'biology', 'nature', 'product design', 'science and art', 'wikipedia', 'community', 'communication', 'gender', 'love', 'psychology', 'relationships', 'theater', 'women', 'astronomy', 'cosmos', 'physics', 'universe', 'choice', 'consumerism', 'food', 'marketing', 'race', 'narcotics', 'decision-making', 'personal growth', 'potential', 'cancer', 'aging', 'biotech', 'future', 'health care', 'investment', 'microfinance', 'poverty', 'telecom', 'transportation', 'corruption', 'military', 'policy', 'NASA', 'aircraft', 'flight', 'rocket science', 'exploration', 'sports', 'travel', 'photography', 'medicine', 'AIDS', 'faith', 'illusion', 'genetics', 'history', 'robots', 'poetry', 'obesity', 'success', 'work', 'anthropology', 'language', 'indigenous peoples', 'complexity', 'time', 'war', 'evolutionary psychology', 'innovation', 'map', 'urban planning', 'United States', 'interview', 'performance art', 'materials', 'code', 'work-life balance', 'ants', 'biodiversity', 'insects', 'ecology', 'biomechanics', 'oceans', 'online video', 'typography', 'graphic design', 'animals', 'biomimicry', 'fish', 'global commons', 'singer', 'apes', 'intelligence', 'Brazil', 'animation', 'primates', 'guitar', 'vocals', 'cello', 'self', 'china', 'memory', 'spoken word', 'web', 'electricity', 'composing', 'natural disaster', 'energy', 'museums', 'water', 'AI', 'marine biology', 'microsoft', 'virtual reality', 'women in business', 'Buddhism', 'New York', 'death', 'terrorism', 'Moon', 'Planets', 'adventure', 'mining', 'space', 'meme', 'bioethics', 'HIV', 'gaming', 'literature', 'markets', 'prosthetics', 'books', 'Social Science', 'sociology', 'violence', 'human origins', 'humanity', 'paleontology', 'solar system', 'asteroid', 'drones', 'solar energy', 'illness', 'depression', 'mental health', 'suicide', 'emotions', 'mindfulness', 'law', 'Best of the Web', 'String theory', 'magic', 'compassion', 'empathy', 'writing', 'play', 'world cultures', 'South America', 'infrastructure', 'ancient world', 'bees', 'garden', 'plants', 'toy', 'telescopes', 'hack', 'heart health', 'public health', 'big bang', 'quantum physics', 'fungi', 'bacteria', 'microbiology', 'news', 'submarine', 'sex', 'society', 'dinosaurs', 'archaeology', 'beauty', 'plastic', 'Vaccines', 'conducting', 'family', 'trees', 'extraterrestrial life', 'astrobiology', 'personality', 'introvert', 'origami', 'dark matter', 'diversity', 'identity', 'nanoscale', 'television', 'geology', 'life', 'morality', 'presentation', 'crime', 'evil', 'prison', 'democracy', 'smell', 'charter for compassion', 'social media', 'Senses', 'Mars', 'fashion', 's\"]', '3D printing', 'goal-setting', 'curiosity', 'programming', 'chemistry', 'shopping', 'body language', 'bionics', 'virus', 'fear', 'birds', 'wind energy', 'extreme sports', 'prediction', 'productivity', 'coral reefs', 'mind', 'TED Fellows', 'natural resources', 'agriculture', 'india', 'neuroscience', 'TEDx', 'money', 'state-building', 'Antarctica', 'Anthropocene', 'Europe', 'data', 'sight', 'journalism', 'Internet', 'government', 'Sun', 'men', 'advertising', 'sanitation', 'homelessness', 'weather', 'big problems', 'rivers', 'Slavery', 'trafficking', 'sexual violence', 'Egypt', 'feminism', 'TEDMED', 'Autism spectrum disorder', 'science fiction', 'botany', 'mission blue', 'friendship', 'nuclear weapons', 'oil', 'novel', 'iraq', 'surveillance', 'Islam', 'monkeys', 'Iran', 'Middle East', 'sound', 'PTSD', 'population', 'manufacturing', 'bullying', 'gender equality', 'trust', 'sleep', 'Foreign Policy', 'medical research', 'Surgery', 'protests', 'deextinction', 'disability', 'nuclear energy', 'driverless cars', 'crowdsourcing', 'Brand', 'speech', 'failure', 'security', 'pain', 'blindness', 'Gender spectrum', 'glacier', 'mobility', 'LGBT', 'public spaces', 'encryption', 'human body', 'nonviolence', 'pharmaceuticals', 'molecular biology', 'behavioral economics', 'physiology', 'medical imaging', 'pregnancy', 'synthetic biology', 'hearing', 'jazz', 'Nobel Prize', 'finance', 'criminal justice', 'justice system', 'algorithm', 'TEDYouth', 'guns', 'exercise', 'conservation', 'immigration', 'TED-Ed', 'privacy', 'microbes', 'machine learning', 'skateboarding', 'augmented reality', 'forensics', 'painting', 'pandemic', 'meditation', 'Syria', 'Transgender', 'student', 'blockchain', 'cryptocurrency', 'Debate', 'farming', 'cloud', 'TED Books', 'refugees', 'street art', 'TED en Español', 'addiction', 'CRISPR', 'vulnerability', 'capitalism', 'grammar', 'Audacious Project', 'resources', 'discovery', 'TEDNYC', 'urban', 'TED Residency', 'biosphere', 's\"', 'epidemiology', 'funny', 'healthcare', 'cooperation', 'stigma', 'Science (hard)', 'neurology', 'arts', 'opioids', 'Humanities', 'book', 'Latin America', 'exoskeleton', 'start-up', 'inclusion', 'gay', 'testing', 'robot', 'human rights', 'development', 'rap', 'coronavirus', 'TED Connects', 'autism']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBxKsFIOg-R"
      },
      "source": [
        "def find_topic(topic):\n",
        "    \"\"\"Returns a list of booleans for talks that contain a topic by index.\n",
        "    \n",
        "    :param topic: Topics or related topics of a talk\n",
        "    \"\"\"\n",
        "    has_topic = []\n",
        "    for t_list in df['topics']:\n",
        "        if topic.lower() in literal_eval(t_list):\n",
        "            has_topic.append(1)\n",
        "        else:\n",
        "            has_topic.append(0)\n",
        "    return has_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p3dPW0WOhE6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "70abf165-8784-46ab-9475-e9aaa390da85"
      },
      "source": [
        "# add columns for selected topics\n",
        "df['is_science'] = find_topic('science')\n",
        "df['is_technology'] = find_topic('technology')\n",
        "df['is_math'] = find_topic('math')\n",
        "df['is_computers'] = find_topic('computers')\n",
        "df['is_engineering'] = find_topic('engineering')\n",
        "df['is_ML'] = find_topic('machine learning')\n",
        "df['is_software'] = find_topic('software')\n",
        "df['is_statistics'] = find_topic('statistics')\n",
        "df['is_cognitive_science'] = find_topic('cognitive science')\n",
        "df['is_science_and_art'] = find_topic('science and art')\n",
        "df['is_physics'] = find_topic('physics')\n",
        "df['is_quantum_physics'] = find_topic('quantum physics')\n",
        "df['is_code'] = find_topic('code')\n",
        "df['is_programming'] = find_topic('programming')\n",
        "df['is_chemistry'] = find_topic('chemistry')\n",
        "df['is_data'] = find_topic('data')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>talk_id</th>\n",
              "      <th>topics</th>\n",
              "      <th>transcript</th>\n",
              "      <th>is_science</th>\n",
              "      <th>is_technology</th>\n",
              "      <th>is_math</th>\n",
              "      <th>is_computers</th>\n",
              "      <th>is_engineering</th>\n",
              "      <th>is_ML</th>\n",
              "      <th>is_software</th>\n",
              "      <th>is_statistics</th>\n",
              "      <th>is_cognitive_science</th>\n",
              "      <th>is_science_and_art</th>\n",
              "      <th>is_physics</th>\n",
              "      <th>is_quantum_physics</th>\n",
              "      <th>is_code</th>\n",
              "      <th>is_programming</th>\n",
              "      <th>is_chemistry</th>\n",
              "      <th>is_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92</td>\n",
              "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
              "      <td>If you're here today — and I'm very happy that...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66</td>\n",
              "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
              "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   talk_id  ... is_data\n",
              "0        1  ...       0\n",
              "1       92  ...       0\n",
              "2        7  ...       0\n",
              "3       53  ...       0\n",
              "4       66  ...       0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxP796sxQWy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34655097-6c0e-4258-8efc-27c54a89016f"
      },
      "source": [
        "# filter DataFrame to only include talks about sex, religion, and politics\n",
        "df = df.loc[(df['is_science']==1) | (df['is_technology']==1) | \n",
        "            (df['is_math']==1) | (df['is_computers']==1) |\n",
        "            (df['is_engineering']==1) | (df['is_ML']==1) | \n",
        "            (df['is_software'] == 1) | (df['is_statistics'] == 1) | \n",
        "            (df['is_cognitive_science'] == 1) | (df['is_science_and_art'] == 1) | \n",
        "            (df['is_physics'] == 1) | (df['is_quantum_physics'] == 1) | \n",
        "            (df['is_code'] == 1) | (df['is_programming'] == 1) | \n",
        "            (df['is_chemistry'] == 1) | df['is_data'] == 1, : ].reset_index(drop=True)\n",
        "\n",
        "# create new DataFrames for each topic (for later use)\n",
        "science_df = df.loc[(df['is_science']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "technology_df = df.loc[(df['is_technology']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "math_df = df.loc[(df['is_math']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "computers_df = df.loc[(df['is_computers']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "engineering_df = df.loc[(df['is_engineering']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "ML_df = df.loc[(df['is_ML']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "software_df = df.loc[(df['is_software']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "statistics_df = df.loc[(df['is_statistics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "cognitive_science_df = df.loc[(df['is_cognitive_science']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "science_and_art_df = df.loc[(df['is_science_and_art']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "physics_df = df.loc[(df['is_physics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "quantum_physics_df = df.loc[(df['is_quantum_physics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "code_df = df.loc[(df['is_code']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "programming_df = df.loc[(df['is_programming']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "chemistry_df = df.loc[(df['is_chemistry']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "data_df = df.loc[(df['is_data']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "\n",
        "print('Science', science_df.shape)\n",
        "print('Technology', technology_df.shape)\n",
        "print('Math', math_df.shape)\n",
        "print('Computers', computers_df.shape)\n",
        "print('Engineering', engineering_df.shape)\n",
        "print('Machine Learning', ML_df.shape)\n",
        "print('Software', software_df.shape)\n",
        "print('Statistics', statistics_df.shape)\n",
        "print('Cognitive Science', cognitive_science_df.shape)\n",
        "print('Science and Art', science_and_art_df.shape)\n",
        "print('Physics', physics_df.shape)\n",
        "print('Quantum Physics', quantum_physics_df.shape)\n",
        "print('Code', code_df.shape)\n",
        "print('Programming', programming_df.shape)\n",
        "print('Chemistry', chemistry_df.shape)\n",
        "print('Data', data_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Science (993, 3)\n",
            "Technology (979, 3)\n",
            "Math (137, 3)\n",
            "Computers (167, 3)\n",
            "Engineering (156, 3)\n",
            "Machine Learning (38, 3)\n",
            "Software (61, 3)\n",
            "Statistics (36, 3)\n",
            "Cognitive Science (71, 3)\n",
            "Science and Art (45, 3)\n",
            "Physics (128, 3)\n",
            "Quantum Physics (17, 3)\n",
            "Code (37, 3)\n",
            "Programming (34, 3)\n",
            "Chemistry (55, 3)\n",
            "Data (142, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6f0IkfAQW5P"
      },
      "source": [
        "def combine_transcripts(transcript_list):\n",
        "    \"\"\"Input a list of transcripts and return them as a corpus.\n",
        "    :param list_of_text: Transcript list\"\"\"\n",
        "    corpus = ' '.join(transcript_list)\n",
        "    return corpus\n",
        "\n",
        "def transcripts_to_dict(df, topic_list):\n",
        "    \"\"\"Returns a dictionary of transcripts for each topic.\n",
        "    \n",
        "    :param df: DataFrame\n",
        "    :param topic_list: List of topics\n",
        "    \"\"\"\n",
        "    ted_dict = {}\n",
        "    for topic in topic_list:\n",
        "        # filter DataFrame to specific series and convert it to a list\n",
        "        filter_string = 'is_' + str(topic)\n",
        "        text_list = df.loc[(df[filter_string] == 1), 'transcript'].to_list()\n",
        "\n",
        "        # call combine_transcripts function to return combined text\n",
        "        combined_text = combine_transcripts(text_list)\n",
        "\n",
        "        # add combined text to dict\n",
        "        ted_dict[topic] = combined_text\n",
        "    return ted_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uKNRdNeQXGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "64cd17f4-cc76-499a-b1c4-016fff076f1e"
      },
      "source": [
        "# create dictionary from the DataFrame\n",
        "transcript_dict = transcripts_to_dict(df, ['science', 'technology', 'math', 'computers', 'engineering', 'ML', \n",
        "                                           'software', 'statistics', 'cognitive_science', 'science_and_art', 'physics', \n",
        "                                           'quantum_physics', 'code', 'programming', 'chemistry', 'data'])\n",
        "\n",
        "# construct DataFrame from dictionary\n",
        "df = pd.DataFrame.from_dict(transcript_dict, orient='index')\n",
        "df.rename({0: 'transcript'}, axis=1, inplace=True)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>science</th>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>technology</th>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>math</th>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computers</th>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>engineering</th>\n",
              "      <td>In terms of invention, I'd like to tell you th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ML</th>\n",
              "      <td>I know this is going to sound strange, but I t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>software</th>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>statistics</th>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cognitive_science</th>\n",
              "      <td>It's wonderful to be back. I love this wonderf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>science_and_art</th>\n",
              "      <td>My name is Lovegrove. I only know nine Lovegro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>physics</th>\n",
              "      <td>My title: \"Queerer than we can suppose: the st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quantum_physics</th>\n",
              "      <td>This is the Large Hadron Collider. It's 27 kil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>code</th>\n",
              "      <td>This meeting has really been about a digital r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>programming</th>\n",
              "      <td>I'm kind of tired of talking about simplicity,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chemistry</th>\n",
              "      <td>This is a wheat bread, a whole wheat bread, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data</th>\n",
              "      <td>I'm going to talk about your mindset. Does you...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          transcript\n",
              "science            Thank you so much, Chris. And it's truly a gre...\n",
              "technology         Thank you so much, Chris. And it's truly a gre...\n",
              "math               About 10 years ago, I took on the task to teac...\n",
              "computers          (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "engineering        In terms of invention, I'd like to tell you th...\n",
              "ML                 I know this is going to sound strange, but I t...\n",
              "software           (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "statistics         About 10 years ago, I took on the task to teac...\n",
              "cognitive_science  It's wonderful to be back. I love this wonderf...\n",
              "science_and_art    My name is Lovegrove. I only know nine Lovegro...\n",
              "physics            My title: \"Queerer than we can suppose: the st...\n",
              "quantum_physics    This is the Large Hadron Collider. It's 27 kil...\n",
              "code               This meeting has really been about a digital r...\n",
              "programming        I'm kind of tired of talking about simplicity,...\n",
              "chemistry          This is a wheat bread, a whole wheat bread, an...\n",
              "data               I'm going to talk about your mindset. Does you..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esArp9P9Qsjh"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Returns clean text.\n",
        "    Removes:\n",
        "        *text in square brackets & parenthesis\n",
        "        *punctuation\n",
        "        *words containing numbers\n",
        "        *double-quotes, dashes\n",
        "    \"\"\"\n",
        "#     text = text.lower()\n",
        "    text = re.sub('[\\[\\(].*?[\\)\\]]', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[\\“\\–]', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjNtcYgDQ7FV"
      },
      "source": [
        "# clean text\n",
        "df['transcript'] = pd.DataFrame(df['transcript'].apply(lambda x: clean_text(x)))\n",
        "science_df['transcript'] = pd.DataFrame(science_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "technology_df['transcript'] = pd.DataFrame(technology_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "math_df['transcript'] = pd.DataFrame(math_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "computers_df['transcript'] = pd.DataFrame(computers_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "engineering_df['transcript'] = pd.DataFrame(engineering_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "ML_df['transcript'] = pd.DataFrame(ML_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "software_df['transcript'] = pd.DataFrame(software_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "statistics_df['transcript'] = pd.DataFrame(statistics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "cognitive_science_df['transcript'] = pd.DataFrame(cognitive_science_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "science_and_art_df['transcript'] = pd.DataFrame(science_and_art_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "physics_df['transcript'] = pd.DataFrame(physics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "quantum_physics_df['transcript'] = pd.DataFrame(quantum_physics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "code_df['transcript'] = pd.DataFrame(code_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "programming_df['transcript'] = pd.DataFrame(programming_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "chemistry_df['transcript'] = pd.DataFrame(chemistry_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "data_df['transcript'] = pd.DataFrame(data_df['transcript'].apply(lambda x: clean_text(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR7dHLSn7jCA"
      },
      "source": [
        "dfs = [science_df, technology_df, math_df, computers_df, engineering_df, ML_df,\n",
        "       software_df, statistics_df, cognitive_science_df, science_and_art_df, physics_df, \n",
        "       quantum_physics_df, code_df, programming_df, chemistry_df, data_df]\n",
        "       \n",
        "comb_df = pd.concat(dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "U5lcp0q8AeLl",
        "outputId": "bf2509ec-7435-46bf-c1bc-ce84f2884486"
      },
      "source": [
        "comb_df.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>talk_id</th>\n",
              "      <th>topics</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>['TED Prize', 'collaboration', 'disease', 'ebo...</td>\n",
              "      <td>I'm the luckiest guy in the world. I got to se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>['cognitive science', 'culture', 'evolution', ...</td>\n",
              "      <td>I'd like to talk today about the two biggest s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>98</td>\n",
              "      <td>['astronomy', 'biology', 'cognitive science', ...</td>\n",
              "      <td>My title: \"Queerer than we can suppose: the st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>47</td>\n",
              "      <td>['climate change', 'cosmos', 'culture', 'envir...</td>\n",
              "      <td>We've been told to go out on a limb and say so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>20554</td>\n",
              "      <td>['communication', 'compassion', 'identity', 'd...</td>\n",
              "      <td>We live in a world where the collection of dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1844</th>\n",
              "      <td>39331</td>\n",
              "      <td>['social change', 'social media', 'democracy',...</td>\n",
              "      <td>So, on the day after the Brexit vote, in June ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>46535</td>\n",
              "      <td>['inequality', 'crime', 'justice system', 'cul...</td>\n",
              "      <td>When people meet me for the first time on my j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1846</th>\n",
              "      <td>53582</td>\n",
              "      <td>['news', 'Internet', 'social media', 'global i...</td>\n",
              "      <td>So, on April  of , the Associated Press put ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>58018</td>\n",
              "      <td>['work', 'gender equality', 'business', 'gende...</td>\n",
              "      <td>A few years ago, I had a corporate feminist dr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1848 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      talk_id  ...                                         transcript\n",
              "0           1  ...  Thank you so much, Chris. And it's truly a gre...\n",
              "1          58  ...  I'm the luckiest guy in the world. I got to se...\n",
              "2          16  ...  I'd like to talk today about the two biggest s...\n",
              "3          98  ...  My title: \"Queerer than we can suppose: the st...\n",
              "4          47  ...  We've been told to go out on a limb and say so...\n",
              "...       ...  ...                                                ...\n",
              "1843    20554  ...  We live in a world where the collection of dat...\n",
              "1844    39331  ...  So, on the day after the Brexit vote, in June ...\n",
              "1845    46535  ...  When people meet me for the first time on my j...\n",
              "1846    53582  ...  So, on April  of , the Associated Press put ou...\n",
              "1847    58018  ...  A few years ago, I had a corporate feminist dr...\n",
              "\n",
              "[1848 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D0fmoeTflon"
      },
      "source": [
        "#comb_df\n",
        "scripts = comb_df[\"transcript\"].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLqAb6RAhPjq"
      },
      "source": [
        "def load_vocab(text):\n",
        "    word_to_ix = {}\n",
        "    for sent in text:\n",
        "        for word in sent.split():\n",
        "            word = word.lower()\n",
        "            word_to_ix.setdefault(word, len(word_to_ix))\n",
        "    return word_to_ix\n",
        "word2idx = load_vocab(scripts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM0AVVqxBUal",
        "outputId": "705ca849-225e-4145-8fc6-e165c827f64a"
      },
      "source": [
        "len(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj5WpOJomJJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496b958c-9db6-43f0-dad5-68cd8332a5e4"
      },
      "source": [
        "# convert the words in list of sentences(scripts) to the corresponding index value(from data)\n",
        "train_data = []\n",
        "for sent in scripts:\n",
        "    words = []\n",
        "    for word in sent.split():\n",
        "        word = word.lower()\n",
        "        words.append(word2idx[word])\n",
        "    train_data.append(words)\n",
        "print(scripts[0]) # as a string\n",
        "print(train_data[0]) # with words replaced by idx value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. And I say that sincerely, partly because  I need that.  Put yourselves in my position.  I flew on Air Force Two for eight years.  Now I have to take off my shoes or boots to get on an airplane!   I'll tell you one quick story to illustrate what that's been like for me.  It's a true story — every bit of this is true. Soon after Tipper and I left the —  White House —  we were driving from our home in Nashville to a little farm we have  miles east of Nashville. Driving ourselves.  I know it sounds like a little thing to you, but —  I looked in the rear-view mirror and all of a sudden it just hit me. There was no motorcade back there.  You've heard of phantom limb pain?  This was a rented Ford Taurus.  It was dinnertime, and we started looking for a place to eat. We were on I-. We got to Exit , Lebanon, Tennessee. We got off the exit, we found a Shoney's restaurant. Low-cost family restaurant chain, for those of you who don't know it. We went in and sat down at the booth, and the waitress came over, made a big commotion over Tipper.  She took our order, and then went to the couple in the booth next to us, and she lowered her voice so much, I had to really strain to hear what she was saying. And she said \"Yes, that's former Vice President Al Gore and his wife, Tipper.\" And the man said, \"He's come down a long way, hasn't he?\"   There's been kind of a series of epiphanies.  The very next day, continuing the totally true story, I got on a G-V to fly to Africa to make a speech in Nigeria, in the city of Lagos, on the topic of energy. And I began the speech by telling them the story of what had just happened the day before in Nashville. And I told it pretty much the same way I've just shared it with you: Tipper and I were driving ourselves, Shoney's, low-cost family restaurant chain, what the man said — they laughed. I gave my speech, then went back out to the airport to fly back home. I fell asleep on the plane until, during the middle of the night, we landed on the Azores Islands for refueling. I woke up, they opened the door, I went out to get some fresh air, and I looked, and there was a man running across the runway. And he was waving a piece of paper, and he was yelling, \"Call Washington! Call Washington!\" And I thought to myself, in the middle of the night, in the middle of the Atlantic, what in the world could be wrong in Washington? Then I remembered it could be a bunch of things.  But what it turned out to be, was that my staff was extremely upset because one of the wire services in Nigeria had already written a story about my speech, and it had already been printed in cities all across the United States of America. It was printed in Monterey, I checked.  And the story began, \"Former Vice President Al Gore announced in Nigeria yesterday,\" quote: 'My wife Tipper and I have opened a low-cost family restaurant'\" —  \"'named Shoney's, and we are running it ourselves.'\"  Before I could get back to U.S. soil, David Letterman and Jay Leno had already started in on — one of them had me in a big white chef's hat, Tipper was saying, \"One more burger with fries!\"  Three days later, I got a nice, long, handwritten letter from my friend and partner and colleague Bill Clinton, saying, \"Congratulations on the new restaurant, Al!\"  We like to celebrate each other's successes in life.  I was going to talk about information ecology. But I was thinking that, since I plan to make a lifelong habit of coming back to TED, that maybe I could talk about that another time.  Chris Anderson: It's a deal!  Al Gore: I want to focus on what many of you have said you would like me to elaborate on: What can you do about the climate crisis? I want to start with a couple of — I'm going to show some new images, and I'm going to recapitulate just four or five. Now, the slide show. I update the slide show every time I give it. I add new images, because I learn more about it every time I give it. It's like beach-combing, you know? Every time the tide comes in and out, you find some more shells. Just in the last two days, we got the new temperature records in January. This is just for the United States of America. Historical average for Januarys is  degrees; last month was . degrees. Now, I know that you wanted some more bad news about the environment — I'm kidding. But these are the recapitulation slides, and then I'm going to go into new material about what you can do. But I wanted to elaborate on a couple of these. First of all, this is where we're projected to go with the U.S. contribution to global warming, under business as usual. Efficiency in end-use electricity and end-use of all energy is the low-hanging fruit. Efficiency and conservation — it's not a cost; it's a profit. The sign is wrong. It's not negative; it's positive. These are investments that pay for themselves. But they are also very effective in deflecting our path. Cars and trucks — I talked about that in the slideshow, but I want you to put it in perspective. It's an easy, visible target of concern — and it should be — but there is more global warming pollution that comes from buildings than from cars and trucks. Cars and trucks are very significant, and we have the lowest standards in the world. And so we should address that. But it's part of the puzzle. Other transportation efficiency is as important as cars and trucks. Renewables at the current levels of technological efficiency can make this much difference. And with what Vinod, and John Doerr and others, many of you here — there are a lot of people directly involved in this — this wedge is going to grow much more rapidly than the current projection shows it. Carbon Capture and Sequestration — that's what CCS stands for — is likely to become the killer app that will enable us to continue to use fossil fuels in a way that is safe. Not quite there yet. OK. Now, what can you do? Reduce emissions in your home. Most of these expenditures are also profitable. Insulation, better design. Buy green electricity where you can. I mentioned automobiles — buy a hybrid. Use light rail. Figure out some of the other options that are much better. It's important. Be a green consumer. You have choices with everything you buy, between things that have a harsh effect, or a much less harsh effect on the global climate crisis. Consider this: Make a decision to live a carbon-neutral life. Those of you who are good at branding, I'd love to get your advice and help on how to say this in a way that connects with the most people. It is easier than you think. It really is. A lot of us in here have made that decision, and it is really pretty easy. It means reduce your carbon dioxide emissions with the full range of choices that you make, and then purchase or acquire offsets for the remainder that you have not completely reduced. And what it means is elaborated at climatecrisis.net. There is a carbon calculator. Participant Productions convened — with my active involvement — the leading software writers in the world, on this arcane science of carbon calculation, to construct a consumer-friendly carbon calculator. You can very precisely calculate what your  emissions are, and then you will be given options to reduce. And by the time the movie comes out in May, this will be updated to ., and we will have click-through purchases of offsets. Next, consider making your business carbon-neutral. Again, some of us have done that, and it's not as hard as you think. Integrate climate solutions into all of your innovations, whether you are from the technology, or entertainment, or design and architecture community. Invest sustainably. Majora mentioned this. Listen, if you have invested money with managers who you compensate on the basis of their annual performance, don't ever again complain about quarterly report CEO management. Over time, people do what you pay them to do. And if they judge how much they're going to get paid on your capital that they've invested, based on the short-term returns, you're going to get short-term decisions. A lot more to be said about that. Become a catalyst of change. Teach others, learn about it, talk about it. The movie is a movie version of the slideshow I gave two nights ago, except it's a lot more entertaining. And it comes out in May. Many of you here have the opportunity to ensure that a lot of people see it. Consider sending somebody to Nashville. Pick well. And I am personally going to train people to give this slideshow — re-purposed, with some of the personal stories obviously replaced with a generic approach, and it's not just the slides, it's what they mean. And it's how they link together. And so I'm going to be conducting a course this summer for a group of people that are nominated by different folks to come and then give it en masse, in communities all across the country, and we're going to update the slideshow for all of them every single week, to keep it right on the cutting edge. Working with Larry Lessig, it will be, somewhere in that process, posted with tools and limited-use copyrights, so that young people can remix it and do it in their own way.  Where did anybody get the idea that you ought to stay arm's length from politics? It doesn't mean that if you're a Republican, that I'm trying to convince you to be a Democrat. We need Republicans as well. This used to be a bipartisan issue, and I know that in this group it really is. Become politically active. Make our democracy work the way it's supposed to work. Support the idea of capping carbon dioxide emissions — global warming pollution — and trading it. Here's why: as long as the United States is out of the world system, it's not a closed system. Once it becomes a closed system, with U.S. participation, then everybody who's on a board of directors — how many people here serve on the board of directors of a corporation? Once it's a closed system, you will have legal liability if you do not urge your CEO to get the maximum income from reducing and trading the carbon emissions that can be avoided. The market will work to solve this problem — if we can accomplish this. Help with the mass persuasion campaign that will start this spring. We have to change the minds of the American people. Because presently, the politicians do not have permission to do what needs to be done. And in our modern country, the role of logic and reason no longer includes mediating between wealth and power the way it once did. It's now repetition of short, hot-button, -second, -second television ads. We have to buy a lot of those ads. Let's re-brand global warming, as many of you have suggested. I like \"climate crisis\" instead of \"climate collapse,\" but again, those of you who are good at branding, I need your help on this. Somebody said the test we're facing now, a scientist told me, is whether the combination of an opposable thumb and a neocortex is a viable combination.  That's really true. I said the other night, and I'll repeat now: this is not a political issue. Again, the Republicans here — this shouldn't be partisan. You have more influence than some of us who are Democrats do. This is an opportunity. Not just this, but connected to the ideas that are here, to bring more coherence to them. We are one. Thank you very much, I appreciate it. \n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 11, 15, 11, 16, 17, 18, 19, 20, 21, 22, 12, 23, 24, 25, 26, 16, 27, 5, 22, 28, 11, 0, 29, 30, 1, 31, 13, 32, 33, 34, 35, 36, 22, 37, 11, 38, 13, 39, 40, 5, 22, 38, 41, 42, 43, 44, 22, 45, 46, 47, 48, 49, 50, 51, 22, 52, 53, 54, 55, 56, 31, 57, 58, 59, 22, 12, 11, 60, 61, 50, 62, 63, 64, 11, 65, 53, 66, 67, 68, 69, 1, 70, 71, 72, 11, 73, 36, 74, 23, 75, 31, 76, 6, 8, 77, 72, 78, 79, 80, 30, 16, 81, 82, 83, 84, 85, 5, 22, 86, 13, 78, 87, 88, 78, 89, 90, 91, 92, 93, 94, 49, 95, 11, 8, 96, 97, 89, 12, 98, 99, 30, 100, 91, 101, 22, 102, 103, 104, 75, 8, 96, 105, 11, 106, 107, 78, 22, 108, 49, 13, 109, 110, 5, 29, 30, 8, 111, 103, 112, 113, 76, 114, 115, 116, 117, 118, 119, 120, 121, 30, 122, 123, 124, 16, 115, 8, 125, 126, 127, 103, 115, 128, 5, 89, 129, 130, 31, 8, 131, 11, 132, 89, 90, 53, 133, 89, 134, 11, 135, 136, 137, 138, 89, 134, 61, 13, 139, 89, 140, 8, 141, 142, 143, 144, 145, 146, 31, 147, 30, 1, 148, 149, 102, 150, 89, 151, 49, 5, 152, 153, 154, 13, 155, 5, 13, 156, 157, 158, 159, 8, 160, 161, 162, 163, 164, 165, 93, 166, 5, 167, 151, 11, 13, 168, 49, 13, 169, 170, 11, 171, 5, 164, 172, 173, 174, 2, 3, 22, 37, 11, 175, 176, 11, 177, 36, 164, 115, 178, 5, 164, 179, 180, 74, 181, 182, 183, 184, 185, 5, 186, 187, 188, 5, 13, 189, 190, 191, 15, 153, 8, 192, 193, 194, 195, 196, 23, 197, 30, 8, 198, 30, 199, 13, 200, 170, 201, 202, 13, 203, 77, 204, 22, 134, 53, 8, 205, 11, 206, 11, 207, 11, 208, 8, 209, 49, 210, 49, 13, 211, 30, 212, 53, 13, 213, 30, 214, 5, 22, 215, 13, 209, 26, 216, 217, 13, 72, 30, 36, 37, 112, 218, 13, 219, 220, 49, 100, 5, 22, 221, 103, 222, 223, 13, 224, 225, 226, 112, 227, 103, 228, 229, 85, 5, 22, 90, 91, 230, 231, 143, 144, 145, 146, 36, 13, 189, 179, 78, 232, 233, 22, 234, 50, 235, 167, 151, 118, 236, 11, 13, 237, 11, 206, 118, 238, 22, 239, 240, 53, 13, 241, 242, 243, 13, 244, 30, 13, 245, 89, 246, 53, 13, 247, 248, 31, 249, 22, 250, 251, 232, 252, 13, 253, 22, 151, 236, 11, 65, 254, 255, 256, 5, 22, 257, 5, 114, 115, 8, 189, 258, 259, 13, 260, 5, 261, 115, 262, 8, 263, 30, 264, 5, 261, 115, 265, 266, 267, 268, 269, 5, 22, 270, 11, 271, 49, 13, 244, 30, 13, 245, 49, 13, 244, 30, 13, 272, 36, 49, 13, 273, 274, 275, 276, 49, 277, 167, 22, 278, 103, 274, 275, 8, 279, 30, 280, 107, 36, 103, 281, 236, 11, 282, 115, 41, 50, 283, 115, 20, 284, 44, 70, 30, 13, 285, 286, 49, 287, 37, 288, 289, 8, 72, 35, 50, 235, 5, 103, 37, 288, 23, 290, 49, 291, 29, 259, 13, 292, 293, 30, 294, 103, 115, 290, 49, 295, 22, 296, 5, 13, 72, 297, 298, 182, 183, 184, 185, 299, 49, 287, 300, 301, 302, 303, 85, 5, 22, 12, 252, 8, 143, 144, 304, 78, 305, 231, 5, 89, 306, 258, 103, 307, 220, 22, 274, 65, 118, 11, 308, 309, 310, 311, 5, 312, 313, 37, 288, 129, 49, 53, 78, 70, 30, 217, 37, 314, 49, 8, 160, 87, 315, 316, 85, 115, 317, 318, 319, 320, 228, 321, 322, 323, 324, 22, 134, 8, 325, 326, 327, 328, 92, 50, 329, 5, 330, 5, 331, 332, 333, 317, 334, 53, 13, 335, 336, 337, 89, 75, 11, 338, 339, 340, 341, 49, 342, 22, 115, 343, 11, 344, 35, 345, 346, 107, 22, 115, 347, 348, 349, 22, 350, 11, 208, 8, 351, 352, 30, 353, 118, 11, 354, 41, 355, 22, 274, 344, 35, 41, 356, 357, 358, 359, 6, 8, 360, 184, 361, 22, 28, 11, 362, 53, 36, 32, 30, 1, 12, 179, 1, 363, 75, 314, 11, 364, 365, 36, 366, 1, 367, 35, 13, 368, 369, 22, 28, 11, 370, 228, 8, 168, 30, 78, 19, 343, 11, 371, 254, 335, 372, 5, 19, 343, 11, 373, 112, 374, 63, 375, 376, 13, 377, 378, 22, 379, 13, 377, 371, 79, 380, 22, 381, 150, 22, 382, 335, 372, 44, 22, 383, 319, 35, 103, 79, 380, 22, 381, 150, 6, 75, 384, 1, 385, 79, 380, 13, 386, 387, 49, 5, 388, 1, 389, 254, 319, 390, 112, 49, 13, 391, 56, 392, 89, 134, 13, 335, 393, 394, 49, 395, 16, 81, 112, 31, 13, 292, 293, 30, 294, 396, 397, 31, 398, 81, 399, 391, 400, 115, 401, 402, 376, 22, 102, 41, 1, 403, 254, 319, 404, 405, 35, 13, 406, 78, 19, 407, 107, 408, 306, 13, 409, 410, 5, 167, 19, 343, 11, 411, 412, 335, 413, 35, 36, 1, 366, 414, 107, 22, 403, 11, 364, 53, 8, 168, 30, 415, 416, 30, 417, 16, 81, 418, 419, 420, 11, 411, 228, 13, 308, 421, 11, 422, 423, 424, 425, 426, 427, 428, 49, 429, 430, 5, 429, 30, 29, 431, 81, 13, 432, 433, 428, 5, 434, 78, 6, 435, 8, 436, 6, 8, 437, 13, 438, 81, 439, 6, 435, 440, 6, 441, 408, 306, 442, 41, 443, 31, 444, 107, 232, 306, 445, 200, 446, 49, 447, 93, 448, 449, 5, 450, 78, 22, 451, 35, 41, 49, 13, 452, 107, 22, 28, 1, 11, 47, 103, 49, 453, 6, 66, 454, 455, 456, 30, 457, 78, 5, 103, 458, 275, 78, 107, 114, 81, 319, 422, 459, 460, 41, 387, 92, 461, 462, 92, 449, 5, 463, 449, 5, 450, 306, 200, 464, 5, 89, 12, 13, 465, 466, 49, 13, 467, 5, 2, 89, 458, 468, 46, 107, 6, 469, 30, 13, 470, 39, 471, 428, 81, 426, 472, 426, 449, 5, 463, 473, 154, 13, 474, 475, 30, 476, 428, 366, 208, 16, 223, 477, 5, 228, 36, 478, 5, 479, 480, 5, 481, 32, 30, 1, 482, 78, 114, 306, 8, 483, 30, 484, 485, 486, 49, 16, 78, 16, 487, 81, 343, 11, 488, 223, 319, 489, 462, 13, 474, 490, 491, 150, 492, 493, 5, 494, 78, 74, 36, 495, 496, 31, 78, 81, 497, 11, 498, 13, 499, 500, 41, 501, 502, 503, 11, 504, 11, 505, 506, 507, 49, 8, 225, 41, 81, 508, 435, 509, 114, 510, 511, 376, 36, 366, 1, 512, 513, 514, 49, 515, 238, 516, 30, 408, 517, 306, 445, 518, 519, 520, 521, 522, 523, 430, 418, 1, 524, 22, 525, 526, 78, 522, 8, 527, 505, 528, 529, 530, 236, 254, 30, 13, 39, 531, 41, 306, 223, 532, 6, 533, 275, 8, 523, 534, 1, 12, 535, 228, 536, 1, 537, 538, 539, 41, 12, 8, 540, 541, 63, 8, 223, 542, 540, 543, 53, 13, 422, 368, 544, 545, 546, 208, 8, 547, 11, 548, 8, 549, 342, 147, 30, 1, 148, 306, 550, 154, 551, 552, 553, 11, 65, 515, 554, 5, 555, 53, 556, 11, 38, 16, 49, 8, 225, 41, 557, 228, 13, 516, 558, 103, 81, 559, 462, 1, 560, 103, 175, 561, 8, 483, 30, 503, 49, 482, 12, 159, 41, 562, 5, 103, 81, 175, 222, 563, 103, 564, 513, 515, 492, 565, 514, 228, 13, 566, 567, 30, 535, 41, 1, 568, 5, 167, 569, 63, 570, 571, 31, 13, 572, 41, 1, 12, 435, 573, 574, 5, 36, 103, 564, 81, 575, 154, 576, 114, 81, 8, 492, 577, 578, 579, 580, 78, 228, 50, 581, 582, 78, 13, 583, 584, 585, 49, 13, 586, 53, 16, 587, 588, 30, 492, 589, 11, 590, 8, 591, 492, 577, 1, 366, 200, 592, 593, 36, 515, 514, 594, 5, 167, 1, 501, 275, 595, 531, 11, 596, 5, 26, 13, 380, 13, 597, 387, 236, 49, 598, 16, 501, 275, 599, 11, 600, 5, 89, 501, 12, 601, 602, 30, 603, 604, 545, 605, 515, 425, 606, 607, 254, 30, 503, 12, 608, 348, 5, 6, 435, 426, 609, 426, 1, 560, 610, 368, 611, 412, 29, 30, 515, 612, 613, 1, 306, 92, 13, 614, 63, 615, 63, 616, 5, 617, 618, 619, 620, 621, 525, 622, 623, 624, 1, 12, 625, 626, 228, 627, 148, 1, 628, 53, 13, 629, 30, 630, 631, 632, 149, 633, 634, 635, 35, 636, 637, 638, 639, 162, 640, 484, 367, 36, 1, 443, 217, 11, 414, 5, 624, 232, 641, 556, 223, 642, 343, 11, 65, 643, 53, 515, 644, 41, 645, 646, 647, 53, 13, 648, 649, 650, 343, 11, 65, 648, 651, 8, 483, 319, 11, 275, 179, 35, 46, 498, 8, 652, 30, 653, 654, 481, 383, 35, 655, 344, 35, 150, 13, 597, 81, 8, 597, 656, 30, 13, 657, 22, 234, 56, 658, 659, 660, 6, 8, 483, 319, 661, 5, 103, 387, 236, 49, 662, 32, 30, 1, 482, 12, 13, 14, 11, 663, 41, 8, 483, 30, 484, 664, 150, 545, 665, 666, 11, 100, 667, 668, 5, 22, 669, 670, 343, 11, 671, 484, 11, 381, 16, 657, 78, 672, 228, 254, 30, 13, 673, 674, 675, 676, 228, 8, 677, 678, 5, 6, 435, 112, 13, 410, 6, 36, 232, 679, 5, 6, 556, 232, 680, 681, 5, 2, 19, 343, 11, 275, 682, 8, 683, 16, 684, 31, 8, 685, 30, 484, 41, 306, 686, 26, 687, 688, 11, 15, 5, 167, 381, 103, 689, 690, 49, 691, 29, 259, 13, 692, 5, 419, 343, 11, 379, 13, 657, 31, 29, 30, 217, 79, 693, 694, 11, 695, 103, 696, 53, 13, 697, 698, 699, 228, 700, 701, 103, 501, 282, 702, 49, 41, 703, 704, 228, 705, 5, 706, 707, 2, 41, 708, 484, 366, 709, 103, 5, 367, 103, 49, 630, 710, 711, 418, 712, 713, 65, 13, 714, 41, 1, 715, 11, 716, 717, 718, 92, 719, 103, 720, 721, 41, 624, 650, 8, 722, 41, 19, 723, 11, 724, 1, 11, 275, 8, 725, 89, 45, 726, 426, 668, 16, 727, 11, 275, 8, 728, 729, 5, 22, 102, 41, 49, 16, 685, 103, 175, 561, 498, 730, 731, 208, 93, 732, 733, 13, 225, 6, 734, 11, 735, 736, 13, 714, 30, 737, 492, 565, 514, 78, 422, 459, 460, 78, 5, 738, 150, 739, 740, 426, 192, 426, 13, 292, 293, 81, 236, 30, 13, 273, 741, 6, 435, 8, 742, 743, 744, 103, 745, 8, 742, 741, 228, 308, 746, 167, 747, 748, 53, 8, 749, 30, 750, 78, 556, 32, 484, 482, 751, 53, 13, 749, 30, 750, 30, 8, 752, 744, 6, 8, 742, 741, 1, 501, 12, 753, 754, 624, 1, 367, 435, 755, 515, 638, 11, 65, 13, 756, 757, 92, 758, 5, 738, 13, 492, 514, 41, 366, 275, 759, 13, 760, 501, 733, 11, 761, 16, 762, 78, 624, 89, 366, 763, 622, 555, 228, 13, 764, 765, 766, 41, 501, 370, 16, 767, 89, 12, 11, 768, 13, 769, 30, 13, 770, 558, 44, 771, 13, 772, 367, 435, 12, 773, 11, 367, 36, 774, 11, 275, 775, 5, 49, 93, 776, 692, 13, 777, 30, 778, 5, 779, 116, 780, 781, 782, 538, 783, 5, 784, 13, 225, 103, 744, 785, 6, 59, 786, 30, 787, 788, 789, 790, 791, 792, 89, 12, 11, 522, 8, 483, 30, 147, 792, 793, 794, 422, 423, 426, 32, 30, 1, 12, 795, 22, 75, 796, 797, 798, 30, 796, 799, 107, 607, 147, 30, 1, 148, 306, 550, 154, 551, 22, 45, 515, 555, 53, 622, 666, 179, 13, 800, 419, 801, 376, 8, 802, 221, 803, 81, 613, 13, 804, 30, 66, 805, 806, 5, 8, 807, 81, 8, 808, 809, 74, 175, 82, 22, 179, 13, 39, 245, 5, 68, 810, 811, 16, 81, 435, 8, 812, 813, 607, 13, 726, 482, 78, 16, 814, 275, 815, 1, 12, 319, 816, 462, 254, 30, 503, 148, 306, 817, 414, 16, 81, 66, 818, 435, 112, 819, 107, 820, 11, 13, 821, 41, 306, 822, 11, 823, 319, 824, 11, 825, 89, 306, 826, 0, 1, 200, 3, 22, 827, 150]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIdj4K9THDq5",
        "outputId": "9adca7d9-ffc6-48b1-ecc4-0eb71fc381f9"
      },
      "source": [
        "# shorten each\n",
        "bs = 20\n",
        "pad = 0 # should be something else since thanks is 0\n",
        "for idx in range(0, len(train_data)):\n",
        "    seqLen = len(train_data[idx])\n",
        "    if seqLen < bs: # # if sequence length not long enough, add padding (make zero for rest of length)\n",
        "        print(\"true\", idx)\n",
        "        [train_data[idx].append(pad) for i in range(bs-seqLen)] # for padding\n",
        "    train_data[idx] = train_data[idx][0:bs] # cut off at sequence length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true 1407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZNjSVEtTP0l",
        "outputId": "285bff6b-dd24-4c35-d1a4-7b0fa911409e"
      },
      "source": [
        "# check if worked\n",
        "idx1 = 5\n",
        "idx2 = 500\n",
        "print(train_data[idx1])\n",
        "print(len(train_data[idx1]))\n",
        "print(train_data[idx2])\n",
        "print(len(train_data[idx2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[869, 1, 12, 3678, 11, 3679, 56, 1002, 924, 2556, 75, 8, 175, 192, 357, 107, 3680, 56, 1002, 924]\n",
            "20\n",
            "[426, 8, 56202, 30, 1726, 11984, 19, 2, 5597, 41, 419, 3956, 5491, 93, 2615, 154, 13, 1751, 30, 18902]\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZcUyi7gTxS"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "end = round(len(train_data)*.8) # to get 80% for training, 20% for testing\n",
        "test_data = torch.Tensor(train_data[end:])\n",
        "train_data = torch.Tensor(train_data[0:end])\n",
        "train_data = train_data.long()\n",
        "test_data = test_data.long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpYI7-0sfUat"
      },
      "source": [
        "class twoLayer_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layers):\n",
        "        super().__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers)\n",
        "        self.lin_layer = nn.Linear(hidden_size, vocab_size)\n",
        "        # if want to make bi directional\n",
        "        #self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers, bidirectional=True)\n",
        "        #self.lin_layer = nn.Linear(hidden_size*2, vocab_size)\n",
        "\n",
        "    def forward(self, word_seq, h_init, c_init):\n",
        "        g_seq = self.emb_layer(word_seq)  \n",
        "        h_seq, (h_last, c_last) = self.rec_layer(g_seq, (h_init, c_init))\n",
        "        score_seq = self.lin_layer(h_seq)\n",
        "        return score_seq, (h_last, c_last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK9nfpHBfN7Y"
      },
      "source": [
        "def eval_on_test_set():\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "    with torch.no_grad():\n",
        "        h = torch.zeros(layers, bs, hidden_size)\n",
        "        c = torch.zeros(layers, bs, hidden_size)\n",
        "        h = h.to(device)\n",
        "        c = c.to(device)\n",
        "\n",
        "        for count in range(0, test_size - seq_length, seq_length):\n",
        "            minibatch_data = test_data[count:count+seq_length]\n",
        "            minibatch_label = test_data[count+1:count+seq_length+1]\n",
        "\n",
        "            minibatch_data = minibatch_data.to(device)\n",
        "            minibatch_label = minibatch_label.to(device)\n",
        "\n",
        "            scores, (h, c) = net(minibatch_data, h, c)\n",
        "\n",
        "            minibatch_label = minibatch_label.view(bs*seq_length) \n",
        "            scores = scores.view(bs*seq_length, vocab_size)\n",
        "\n",
        "            loss = criterion(scores, minibatch_label)    \n",
        "\n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "\n",
        "            #running_loss += loss.item()\n",
        "            num_batches += 1        \n",
        "    \n",
        "    #total_loss = running_loss/num_batches \n",
        "    print('test loss =', loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtMBtSLcfFPO"
      },
      "source": [
        "# setup NN\n",
        "hidden_size = 200\n",
        "vocab_size = len(word2idx)\n",
        "layers = 2\n",
        "num_epoch = 40\n",
        "seq_length = bs\n",
        "my_lr = 0.1\n",
        "\n",
        "net = twoLayer_LSTM(vocab_size, hidden_size, layers)\n",
        "#net.emb_layer.weight.data.uniform_(-0.1, 0.1)\n",
        "#net.lin_layer.weight = net.emb_layer.weight\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_size = len(train_data)\n",
        "test_size = len(test_data)\n",
        "optimizer = optim.SGD(net.parameters(), lr=my_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ47Sb7dCCMo"
      },
      "source": [
        "def normalize_gradient(net):\n",
        "    grad_norm_sq = 0\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "    grad_norm = math.sqrt(grad_norm_sq)\n",
        "    if grad_norm < 1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "             p.grad.data.div_(grad_norm)\n",
        "    return grad_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovkg4U3CYUMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88043a36-299b-4613-9ad2-0a6c6bb92a71"
      },
      "source": [
        "# training\n",
        "start = time.time()\n",
        "#n = 0.5\n",
        "for epoch in range(num_epoch):\n",
        "    #if epoch >= 4: \n",
        "        #if my_lr > 0.06:\n",
        "            #my_lr = my_lr/(4*n)\n",
        "        #n += 0.5\n",
        "        #optimizer = optim.SGD(net.parameters(), lr=my_lr) \n",
        "            \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "       \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(layers, bs, hidden_size)\n",
        "    c = torch.zeros(layers, bs, hidden_size)\n",
        "    # send it to the gpu    \n",
        "    h = h.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    for count in range(0, train_size - seq_length, seq_length):    \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data = train_data[count:count+seq_length]\n",
        "        minibatch_label = train_data[count+1:count+seq_length+1]        \n",
        "                \n",
        "        # send them to the gpu\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h = h.detach()\n",
        "        c = c.detach()\n",
        "        h = h.requires_grad_()\n",
        "        c = c.requires_grad_()\n",
        "        # forward the minibatch through the net \n",
        "        scores, (h, c) = net(minibatch_data, h, c)\n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores = scores.view(bs*seq_length, vocab_size)  \n",
        "        minibatch_label = minibatch_label.view(bs*seq_length)       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update the running loss  \n",
        "        #running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "                          \n",
        "    #total_loss = running_loss/num_batches\n",
        "    elapsed = time.time() - start\n",
        "    #print('\\nepoch =', epoch, '\\t time =', elapsed,'\\t lr =', my_lr, '\\t exp(loss) =', math.exp(total_loss)) # compute error on the test set at end of each epoch\n",
        "    print('\\nepoch =', epoch, '\\t time =', elapsed,'\\t lr =', my_lr, '\\t training loss =', loss.item()) # compute error on the test set at end of each epoch\n",
        "    eval_on_test_set() \n",
        "\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch = 0 \t time = 4.903045654296875 \t lr = 0.1 \t training loss = 8.48658561706543\n",
            "test loss = 8.5008544921875\n",
            "\n",
            "epoch = 1 \t time = 10.27333688735962 \t lr = 0.1 \t training loss = 7.898642063140869\n",
            "test loss = 7.949413299560547\n",
            "\n",
            "epoch = 2 \t time = 15.664945840835571 \t lr = 0.1 \t training loss = 7.664650917053223\n",
            "test loss = 7.704073905944824\n",
            "\n",
            "epoch = 3 \t time = 21.057207107543945 \t lr = 0.1 \t training loss = 7.51109504699707\n",
            "test loss = 7.561974048614502\n",
            "\n",
            "epoch = 4 \t time = 26.469321727752686 \t lr = 0.1 \t training loss = 7.393784999847412\n",
            "test loss = 7.465356826782227\n",
            "\n",
            "epoch = 5 \t time = 31.882391929626465 \t lr = 0.1 \t training loss = 7.299864292144775\n",
            "test loss = 7.396674156188965\n",
            "\n",
            "epoch = 6 \t time = 37.27615189552307 \t lr = 0.1 \t training loss = 7.223617076873779\n",
            "test loss = 7.346992015838623\n",
            "\n",
            "epoch = 7 \t time = 42.650012254714966 \t lr = 0.1 \t training loss = 7.160800933837891\n",
            "test loss = 7.309502601623535\n",
            "\n",
            "epoch = 8 \t time = 48.024611949920654 \t lr = 0.1 \t training loss = 7.108007431030273\n",
            "test loss = 7.2797369956970215\n",
            "\n",
            "epoch = 9 \t time = 53.3990523815155 \t lr = 0.1 \t training loss = 7.062862396240234\n",
            "test loss = 7.255235195159912\n",
            "\n",
            "epoch = 10 \t time = 58.77739429473877 \t lr = 0.1 \t training loss = 7.023782730102539\n",
            "test loss = 7.234614372253418\n",
            "\n",
            "epoch = 11 \t time = 64.02840781211853 \t lr = 0.1 \t training loss = 6.9896559715271\n",
            "test loss = 7.216975688934326\n",
            "\n",
            "epoch = 12 \t time = 69.2765679359436 \t lr = 0.1 \t training loss = 6.959622383117676\n",
            "test loss = 7.2016825675964355\n",
            "\n",
            "epoch = 13 \t time = 74.64704442024231 \t lr = 0.1 \t training loss = 6.932989120483398\n",
            "test loss = 7.1882524490356445\n",
            "\n",
            "epoch = 14 \t time = 80.0211021900177 \t lr = 0.1 \t training loss = 6.909188747406006\n",
            "test loss = 7.176324367523193\n",
            "\n",
            "epoch = 15 \t time = 85.39477968215942 \t lr = 0.1 \t training loss = 6.887775897979736\n",
            "test loss = 7.165632247924805\n",
            "\n",
            "epoch = 16 \t time = 90.76626086235046 \t lr = 0.1 \t training loss = 6.8684000968933105\n",
            "test loss = 7.155978202819824\n",
            "\n",
            "epoch = 17 \t time = 96.13720655441284 \t lr = 0.1 \t training loss = 6.850788116455078\n",
            "test loss = 7.147210597991943\n",
            "\n",
            "epoch = 18 \t time = 101.50972008705139 \t lr = 0.1 \t training loss = 6.83473014831543\n",
            "test loss = 7.139215469360352\n",
            "\n",
            "epoch = 19 \t time = 106.88312792778015 \t lr = 0.1 \t training loss = 6.820057392120361\n",
            "test loss = 7.131898880004883\n",
            "\n",
            "epoch = 20 \t time = 112.25335264205933 \t lr = 0.1 \t training loss = 6.806636333465576\n",
            "test loss = 7.125184535980225\n",
            "\n",
            "epoch = 21 \t time = 117.62831139564514 \t lr = 0.1 \t training loss = 6.7943549156188965\n",
            "test loss = 7.119003772735596\n",
            "\n",
            "epoch = 22 \t time = 123.00547409057617 \t lr = 0.1 \t training loss = 6.783113956451416\n",
            "test loss = 7.113299369812012\n",
            "\n",
            "epoch = 23 \t time = 128.38266849517822 \t lr = 0.1 \t training loss = 6.772830009460449\n",
            "test loss = 7.1080193519592285\n",
            "\n",
            "epoch = 24 \t time = 133.73621368408203 \t lr = 0.1 \t training loss = 6.763427734375\n",
            "test loss = 7.103121757507324\n",
            "\n",
            "epoch = 25 \t time = 139.10766649246216 \t lr = 0.1 \t training loss = 6.754832744598389\n",
            "test loss = 7.098563194274902\n",
            "\n",
            "epoch = 26 \t time = 144.4795377254486 \t lr = 0.1 \t training loss = 6.746983528137207\n",
            "test loss = 7.094311714172363\n",
            "\n",
            "epoch = 27 \t time = 149.85171461105347 \t lr = 0.1 \t training loss = 6.739819526672363\n",
            "test loss = 7.090334892272949\n",
            "\n",
            "epoch = 28 \t time = 155.22444987297058 \t lr = 0.1 \t training loss = 6.733282089233398\n",
            "test loss = 7.086608409881592\n",
            "\n",
            "epoch = 29 \t time = 160.59442234039307 \t lr = 0.1 \t training loss = 6.727321147918701\n",
            "test loss = 7.083108425140381\n",
            "\n",
            "epoch = 30 \t time = 165.9650158882141 \t lr = 0.1 \t training loss = 6.721886157989502\n",
            "test loss = 7.079812526702881\n",
            "\n",
            "epoch = 31 \t time = 171.31826996803284 \t lr = 0.1 \t training loss = 6.7169270515441895\n",
            "test loss = 7.0767035484313965\n",
            "\n",
            "epoch = 32 \t time = 176.69228076934814 \t lr = 0.1 \t training loss = 6.712405204772949\n",
            "test loss = 7.07376766204834\n",
            "\n",
            "epoch = 33 \t time = 182.06625843048096 \t lr = 0.1 \t training loss = 6.708277702331543\n",
            "test loss = 7.070988178253174\n",
            "\n",
            "epoch = 34 \t time = 187.43972420692444 \t lr = 0.1 \t training loss = 6.704510688781738\n",
            "test loss = 7.068350315093994\n",
            "\n",
            "epoch = 35 \t time = 192.81354999542236 \t lr = 0.1 \t training loss = 6.701066970825195\n",
            "test loss = 7.0658488273620605\n",
            "\n",
            "epoch = 36 \t time = 198.19091534614563 \t lr = 0.1 \t training loss = 6.697917461395264\n",
            "test loss = 7.063469886779785\n",
            "\n",
            "epoch = 37 \t time = 203.56220364570618 \t lr = 0.1 \t training loss = 6.695033073425293\n",
            "test loss = 7.061203479766846\n",
            "\n",
            "epoch = 38 \t time = 208.93323922157288 \t lr = 0.1 \t training loss = 6.692389488220215\n",
            "test loss = 7.059044361114502\n",
            "\n",
            "epoch = 39 \t time = 214.3058443069458 \t lr = 0.1 \t training loss = 6.689962863922119\n",
            "test loss = 7.056983470916748\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCCdbrCtHyrn"
      },
      "source": [
        "idx2word = {y:x for x, y in word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am1geAsSGi9L"
      },
      "source": [
        "def show_most_likely_words(prob):\n",
        "    num_word_display = 30\n",
        "    p = prob.view(-1)\n",
        "    p, word_idx = torch.topk(p, num_word_display)\n",
        "    for i, idx in enumerate(word_idx):\n",
        "        percentage = p[i].item()*100\n",
        "        word = idx2word[idx.item()]\n",
        "        print(\"{:.1f}%\\t\".format(percentage), word) \n",
        "\n",
        "def text2tensor(text):\n",
        "    text = text.lower()\n",
        "    list_of_words = text.split()\n",
        "    list_of_int = [word2idx[w] for w in list_of_words]\n",
        "    x = torch.LongTensor(list_of_int)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Tu0gW1CKBU",
        "outputId": "dc91953e-ceaa-4768-860c-ab3c5b158b95"
      },
      "source": [
        "sentence = \"machine learning is\"\n",
        "\n",
        "h = torch.zeros(layers, bs, hidden_size)\n",
        "c = torch.zeros(layers, bs, hidden_size)\n",
        "h = h.to(device)\n",
        "c = c.to(device)\n",
        "\n",
        "data = text2tensor(sentence)\n",
        "seq_length = len(data)\n",
        "data = data.view(seq_length, -1)\n",
        "empty = torch.zeros(seq_length, 19).type(torch.LongTensor)\n",
        "data = torch.cat((data, empty), dim=1)\n",
        "data = data.to(device)\n",
        "scores, (h, c) = net(data, h, c)\n",
        "scores = scores[seq_length-1, 0, :]\n",
        "p = F.softmax(scores.view(1, vocab_size), dim=1)\n",
        "print(sentence, '... \\n')\n",
        "show_most_likely_words(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine learning is ... \n",
            "\n",
            "0.1%\t a\n",
            "0.1%\t the\n",
            "0.1%\t to\n",
            "0.1%\t and\n",
            "0.1%\t of\n",
            "0.1%\t i\n",
            "0.1%\t in\n",
            "0.1%\t you\n",
            "0.1%\t is\n",
            "0.0%\t about\n",
            "0.0%\t we\n",
            "0.0%\t that\n",
            "0.0%\t this\n",
            "0.0%\t was\n",
            "0.0%\t have\n",
            "0.0%\t i'm\n",
            "0.0%\t with\n",
            "0.0%\t it's\n",
            "0.0%\t my\n",
            "0.0%\t it\n",
            "0.0%\t so\n",
            "0.0%\t for\n",
            "0.0%\t an\n",
            "0.0%\t on\n",
            "0.0%\t like\n",
            "0.0%\t are\n",
            "0.0%\t be\n",
            "0.0%\t at\n",
            "0.0%\t from\n",
            "0.0%\t been\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbsvQ9jiGaWH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}