{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of baseline_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UgKbXYvqPjZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from ast import literal_eval\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3fFEVH58Og0T",
        "outputId": "bcaee739-e4f0-4eff-958e-c0cdb8ef71a6"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lmu-mandy/project-rgt/bob-branch/ted_talks_en.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df = df.loc[:, ['talk_id', 'topics', 'transcript']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>talk_id</th>\n",
              "      <th>topics</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92</td>\n",
              "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
              "      <td>If you're here today — and I'm very happy that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66</td>\n",
              "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
              "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   talk_id  ...                                         transcript\n",
              "0        1  ...  Thank you so much, Chris. And it's truly a gre...\n",
              "1       92  ...  About 10 years ago, I took on the task to teac...\n",
              "2        7  ...  (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "3       53  ...  If you're here today — and I'm very happy that...\n",
              "4       66  ...  Good morning. How are you? (Audience) Good. It...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRHtVN-7fgNT",
        "outputId": "44489a9a-c5c0-4c6a-faee-f2702bff850c"
      },
      "source": [
        "sep_topics = df.topics.unique()\n",
        "topics = []\n",
        "\n",
        "for topic in sep_topics:\n",
        "    for i in topic.split(\",\"):\n",
        "        topics.append(i.split(\"'\")[1])\n",
        "print(topics[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alternative energy', 'cars', 'climate change', 'culture', 'environment']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2uwR-X-uUW",
        "outputId": "aaa9eaf2-d737-4945-fa60-da64837a11a0"
      },
      "source": [
        "unique_topics = [] \n",
        "      \n",
        "# traverse for all elements \n",
        "for topic in topics: \n",
        "    # check if exists in unique_list or not \n",
        "    if topic not in unique_topics: \n",
        "            unique_topics.append(topic) \n",
        "print(unique_topics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alternative energy', 'cars', 'climate change', 'culture', 'environment', 'global issues', 'science', 'sustainability', 'technology', 'Africa', 'Asia', 'Google', 'demo', 'economics', 'health', 'statistics', 'global development', 'visualizations', 'math', 'computers', 'entertainment', 'interface design', 'media', 'music', 'performance', 'simplicity', 'software', 'MacArthur grant', 'activism', 'business', 'cities', 'green', 'inequality', 'politics', 'pollution', 'children', 'creativity', 'dance', 'education', 'parenting', 'teaching', 'architecture', 'collaboration', 'design', 'library', 'Christianity', 'God', 'atheism', 'comedy', 'religion', 'storytelling', 'humor', 'brain', 'cognitive science', 'consciousness', 'evolution', 'philosophy', 'happiness', 'leadership', 'motivation', 'philanthropy', 'TED Prize', 'film', 'peace', 'social change', 'art', 'movies', 'disease', 'ebola', 'disaster relief', 'invention', 'open-source', 'entrepreneur', 'piano', 'wunderkind', 'live music', 'violin', 'youth', 'engineering', 'industrial design', 'DNA', 'biology', 'nature', 'product design', 'science and art', 'wikipedia', 'community', 'communication', 'gender', 'love', 'psychology', 'relationships', 'theater', 'women', 'astronomy', 'cosmos', 'physics', 'universe', 'choice', 'consumerism', 'food', 'marketing', 'race', 'narcotics', 'decision-making', 'personal growth', 'potential', 'cancer', 'aging', 'biotech', 'future', 'health care', 'investment', 'microfinance', 'poverty', 'telecom', 'transportation', 'corruption', 'military', 'policy', 'NASA', 'aircraft', 'flight', 'rocket science', 'exploration', 'sports', 'travel', 'photography', 'medicine', 'AIDS', 'faith', 'illusion', 'genetics', 'history', 'robots', 'poetry', 'obesity', 'success', 'work', 'anthropology', 'language', 'indigenous peoples', 'complexity', 'time', 'war', 'evolutionary psychology', 'innovation', 'map', 'urban planning', 'United States', 'interview', 'performance art', 'materials', 'code', 'work-life balance', 'ants', 'biodiversity', 'insects', 'ecology', 'biomechanics', 'oceans', 'online video', 'typography', 'graphic design', 'animals', 'biomimicry', 'fish', 'global commons', 'singer', 'apes', 'intelligence', 'Brazil', 'animation', 'primates', 'guitar', 'vocals', 'cello', 'self', 'china', 'memory', 'spoken word', 'web', 'electricity', 'composing', 'natural disaster', 'energy', 'museums', 'water', 'AI', 'marine biology', 'microsoft', 'virtual reality', 'women in business', 'Buddhism', 'New York', 'death', 'terrorism', 'Moon', 'Planets', 'adventure', 'mining', 'space', 'meme', 'bioethics', 'HIV', 'gaming', 'literature', 'markets', 'prosthetics', 'books', 'Social Science', 'sociology', 'violence', 'human origins', 'humanity', 'paleontology', 'solar system', 'asteroid', 'drones', 'solar energy', 'illness', 'depression', 'mental health', 'suicide', 'emotions', 'mindfulness', 'law', 'Best of the Web', 'String theory', 'magic', 'compassion', 'empathy', 'writing', 'play', 'world cultures', 'South America', 'infrastructure', 'ancient world', 'bees', 'garden', 'plants', 'toy', 'telescopes', 'hack', 'heart health', 'public health', 'big bang', 'quantum physics', 'fungi', 'bacteria', 'microbiology', 'news', 'submarine', 'sex', 'society', 'dinosaurs', 'archaeology', 'beauty', 'plastic', 'Vaccines', 'conducting', 'family', 'trees', 'extraterrestrial life', 'astrobiology', 'personality', 'introvert', 'origami', 'dark matter', 'diversity', 'identity', 'nanoscale', 'television', 'geology', 'life', 'morality', 'presentation', 'crime', 'evil', 'prison', 'democracy', 'smell', 'charter for compassion', 'social media', 'Senses', 'Mars', 'fashion', 's\"]', '3D printing', 'goal-setting', 'curiosity', 'programming', 'chemistry', 'shopping', 'body language', 'bionics', 'virus', 'fear', 'birds', 'wind energy', 'extreme sports', 'prediction', 'productivity', 'coral reefs', 'mind', 'TED Fellows', 'natural resources', 'agriculture', 'india', 'neuroscience', 'TEDx', 'money', 'state-building', 'Antarctica', 'Anthropocene', 'Europe', 'data', 'sight', 'journalism', 'Internet', 'government', 'Sun', 'men', 'advertising', 'sanitation', 'homelessness', 'weather', 'big problems', 'rivers', 'Slavery', 'trafficking', 'sexual violence', 'Egypt', 'feminism', 'TEDMED', 'Autism spectrum disorder', 'science fiction', 'botany', 'mission blue', 'friendship', 'nuclear weapons', 'oil', 'novel', 'iraq', 'surveillance', 'Islam', 'monkeys', 'Iran', 'Middle East', 'sound', 'PTSD', 'population', 'manufacturing', 'bullying', 'gender equality', 'trust', 'sleep', 'Foreign Policy', 'medical research', 'Surgery', 'protests', 'deextinction', 'disability', 'nuclear energy', 'driverless cars', 'crowdsourcing', 'Brand', 'speech', 'failure', 'security', 'pain', 'blindness', 'Gender spectrum', 'glacier', 'mobility', 'LGBT', 'public spaces', 'encryption', 'human body', 'nonviolence', 'pharmaceuticals', 'molecular biology', 'behavioral economics', 'physiology', 'medical imaging', 'pregnancy', 'synthetic biology', 'hearing', 'jazz', 'Nobel Prize', 'finance', 'criminal justice', 'justice system', 'algorithm', 'TEDYouth', 'guns', 'exercise', 'conservation', 'immigration', 'TED-Ed', 'privacy', 'microbes', 'machine learning', 'skateboarding', 'augmented reality', 'forensics', 'painting', 'pandemic', 'meditation', 'Syria', 'Transgender', 'student', 'blockchain', 'cryptocurrency', 'Debate', 'farming', 'cloud', 'TED Books', 'refugees', 'street art', 'TED en Español', 'addiction', 'CRISPR', 'vulnerability', 'capitalism', 'grammar', 'Audacious Project', 'resources', 'discovery', 'TEDNYC', 'urban', 'TED Residency', 'biosphere', 's\"', 'epidemiology', 'funny', 'healthcare', 'cooperation', 'stigma', 'Science (hard)', 'neurology', 'arts', 'opioids', 'Humanities', 'book', 'Latin America', 'exoskeleton', 'start-up', 'inclusion', 'gay', 'testing', 'robot', 'human rights', 'development', 'rap', 'coronavirus', 'TED Connects', 'autism']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBxKsFIOg-R"
      },
      "source": [
        "def find_topic(topic):\n",
        "    \"\"\"Returns a list of booleans for talks that contain a topic by index.\n",
        "    \n",
        "    :param topic: Topics or related topics of a talk\n",
        "    \"\"\"\n",
        "    has_topic = []\n",
        "    for t_list in df['topics']:\n",
        "        if topic.lower() in literal_eval(t_list):\n",
        "            has_topic.append(1)\n",
        "        else:\n",
        "            has_topic.append(0)\n",
        "    return has_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "4p3dPW0WOhE6",
        "outputId": "0b475ef2-af14-412c-c9aa-898932667d6b"
      },
      "source": [
        "# add columns for selected topics\n",
        "df['is_science'] = find_topic('science')\n",
        "df['is_technology'] = find_topic('technology')\n",
        "df['is_math'] = find_topic('math')\n",
        "df['is_computers'] = find_topic('computers')\n",
        "df['is_engineering'] = find_topic('engineering')\n",
        "df['is_ML'] = find_topic('machine learning')\n",
        "df['is_software'] = find_topic('software')\n",
        "df['is_statistics'] = find_topic('statistics')\n",
        "df['is_cognitive_science'] = find_topic('cognitive science')\n",
        "df['is_science_and_art'] = find_topic('science and art')\n",
        "df['is_physics'] = find_topic('physics')\n",
        "df['is_quantum_physics'] = find_topic('quantum physics')\n",
        "df['is_code'] = find_topic('code')\n",
        "df['is_programming'] = find_topic('programming')\n",
        "df['is_chemistry'] = find_topic('chemistry')\n",
        "df['is_data'] = find_topic('data')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>talk_id</th>\n",
              "      <th>topics</th>\n",
              "      <th>transcript</th>\n",
              "      <th>is_science</th>\n",
              "      <th>is_technology</th>\n",
              "      <th>is_math</th>\n",
              "      <th>is_computers</th>\n",
              "      <th>is_engineering</th>\n",
              "      <th>is_ML</th>\n",
              "      <th>is_software</th>\n",
              "      <th>is_statistics</th>\n",
              "      <th>is_cognitive_science</th>\n",
              "      <th>is_science_and_art</th>\n",
              "      <th>is_physics</th>\n",
              "      <th>is_quantum_physics</th>\n",
              "      <th>is_code</th>\n",
              "      <th>is_programming</th>\n",
              "      <th>is_chemistry</th>\n",
              "      <th>is_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92</td>\n",
              "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
              "      <td>If you're here today — and I'm very happy that...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66</td>\n",
              "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
              "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   talk_id  ... is_data\n",
              "0        1  ...       0\n",
              "1       92  ...       0\n",
              "2        7  ...       0\n",
              "3       53  ...       0\n",
              "4       66  ...       0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxP796sxQWy5",
        "outputId": "81cf8b89-6741-4f97-dae5-d485307b5d4e"
      },
      "source": [
        "# filter DataFrame to only include talks about sex, religion, and politics\n",
        "df = df.loc[(df['is_science']==1) | (df['is_technology']==1) | \n",
        "            (df['is_math']==1) | (df['is_computers']==1) |\n",
        "            (df['is_engineering']==1) | (df['is_ML']==1) | \n",
        "            (df['is_software'] == 1) | (df['is_statistics'] == 1) | \n",
        "            (df['is_cognitive_science'] == 1) | (df['is_science_and_art'] == 1) | \n",
        "            (df['is_physics'] == 1) | (df['is_quantum_physics'] == 1) | \n",
        "            (df['is_code'] == 1) | (df['is_programming'] == 1) | \n",
        "            (df['is_chemistry'] == 1) | df['is_data'] == 1, : ].reset_index(drop=True)\n",
        "\n",
        "# create new DataFrames for each topic (for later use)\n",
        "science_df = df.loc[(df['is_science']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "technology_df = df.loc[(df['is_technology']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "math_df = df.loc[(df['is_math']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "computers_df = df.loc[(df['is_computers']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "engineering_df = df.loc[(df['is_engineering']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "ML_df = df.loc[(df['is_ML']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "software_df = df.loc[(df['is_software']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "statistics_df = df.loc[(df['is_statistics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "cognitive_science_df = df.loc[(df['is_cognitive_science']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "science_and_art_df = df.loc[(df['is_science_and_art']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "physics_df = df.loc[(df['is_physics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "quantum_physics_df = df.loc[(df['is_quantum_physics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "code_df = df.loc[(df['is_code']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "programming_df = df.loc[(df['is_programming']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "chemistry_df = df.loc[(df['is_chemistry']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "data_df = df.loc[(df['is_data']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "\n",
        "print('Science', science_df.shape)\n",
        "print('Technology', technology_df.shape)\n",
        "print('Math', math_df.shape)\n",
        "print('Computers', computers_df.shape)\n",
        "print('Engineering', engineering_df.shape)\n",
        "print('Machine Learning', ML_df.shape)\n",
        "print('Software', software_df.shape)\n",
        "print('Statistics', statistics_df.shape)\n",
        "print('Cognitive Science', cognitive_science_df.shape)\n",
        "print('Science and Art', science_and_art_df.shape)\n",
        "print('Physics', physics_df.shape)\n",
        "print('Quantum Physics', quantum_physics_df.shape)\n",
        "print('Code', code_df.shape)\n",
        "print('Programming', programming_df.shape)\n",
        "print('Chemistry', chemistry_df.shape)\n",
        "print('Data', data_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Science (993, 3)\n",
            "Technology (979, 3)\n",
            "Math (137, 3)\n",
            "Computers (167, 3)\n",
            "Engineering (156, 3)\n",
            "Machine Learning (38, 3)\n",
            "Software (61, 3)\n",
            "Statistics (36, 3)\n",
            "Cognitive Science (71, 3)\n",
            "Science and Art (45, 3)\n",
            "Physics (128, 3)\n",
            "Quantum Physics (17, 3)\n",
            "Code (37, 3)\n",
            "Programming (34, 3)\n",
            "Chemistry (55, 3)\n",
            "Data (142, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6f0IkfAQW5P"
      },
      "source": [
        "def combine_transcripts(transcript_list):\n",
        "    \"\"\"Input a list of transcripts and return them as a corpus.\n",
        "    :param list_of_text: Transcript list\"\"\"\n",
        "    corpus = ' '.join(transcript_list)\n",
        "    return corpus\n",
        "\n",
        "def transcripts_to_dict(df, topic_list):\n",
        "    \"\"\"Returns a dictionary of transcripts for each topic.\n",
        "    \n",
        "    :param df: DataFrame\n",
        "    :param topic_list: List of topics\n",
        "    \"\"\"\n",
        "    ted_dict = {}\n",
        "    for topic in topic_list:\n",
        "        # filter DataFrame to specific series and convert it to a list\n",
        "        filter_string = 'is_' + str(topic)\n",
        "        text_list = df.loc[(df[filter_string] == 1), 'transcript'].to_list()\n",
        "\n",
        "        # call combine_transcripts function to return combined text\n",
        "        combined_text = combine_transcripts(text_list)\n",
        "\n",
        "        # add combined text to dict\n",
        "        ted_dict[topic] = combined_text\n",
        "    return ted_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "1uKNRdNeQXGR",
        "outputId": "d54ecf72-9230-4b34-a431-7be3e3b3c85d"
      },
      "source": [
        "# create dictionary from the DataFrame\n",
        "transcript_dict = transcripts_to_dict(df, ['science', 'technology', 'math', 'computers', 'engineering', 'ML', \n",
        "                                           'software', 'statistics', 'cognitive_science', 'science_and_art', 'physics', \n",
        "                                           'quantum_physics', 'code', 'programming', 'chemistry', 'data'])\n",
        "\n",
        "# construct DataFrame from dictionary\n",
        "df = pd.DataFrame.from_dict(transcript_dict, orient='index')\n",
        "df.rename({0: 'transcript'}, axis=1, inplace=True)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>science</th>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>technology</th>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>math</th>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computers</th>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>engineering</th>\n",
              "      <td>In terms of invention, I'd like to tell you th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ML</th>\n",
              "      <td>I know this is going to sound strange, but I t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>software</th>\n",
              "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>statistics</th>\n",
              "      <td>About 10 years ago, I took on the task to teac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cognitive_science</th>\n",
              "      <td>It's wonderful to be back. I love this wonderf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>science_and_art</th>\n",
              "      <td>My name is Lovegrove. I only know nine Lovegro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>physics</th>\n",
              "      <td>My title: \"Queerer than we can suppose: the st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quantum_physics</th>\n",
              "      <td>This is the Large Hadron Collider. It's 27 kil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>code</th>\n",
              "      <td>This meeting has really been about a digital r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>programming</th>\n",
              "      <td>I'm kind of tired of talking about simplicity,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chemistry</th>\n",
              "      <td>This is a wheat bread, a whole wheat bread, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data</th>\n",
              "      <td>I'm going to talk about your mindset. Does you...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          transcript\n",
              "science            Thank you so much, Chris. And it's truly a gre...\n",
              "technology         Thank you so much, Chris. And it's truly a gre...\n",
              "math               About 10 years ago, I took on the task to teac...\n",
              "computers          (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "engineering        In terms of invention, I'd like to tell you th...\n",
              "ML                 I know this is going to sound strange, but I t...\n",
              "software           (Music: \"The Sound of Silence,\" Simon & Garfun...\n",
              "statistics         About 10 years ago, I took on the task to teac...\n",
              "cognitive_science  It's wonderful to be back. I love this wonderf...\n",
              "science_and_art    My name is Lovegrove. I only know nine Lovegro...\n",
              "physics            My title: \"Queerer than we can suppose: the st...\n",
              "quantum_physics    This is the Large Hadron Collider. It's 27 kil...\n",
              "code               This meeting has really been about a digital r...\n",
              "programming        I'm kind of tired of talking about simplicity,...\n",
              "chemistry          This is a wheat bread, a whole wheat bread, an...\n",
              "data               I'm going to talk about your mindset. Does you..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esArp9P9Qsjh"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Returns clean text.\n",
        "    Removes:\n",
        "        *text in square brackets & parenthesis\n",
        "        *punctuation\n",
        "        *words containing numbers\n",
        "        *double-quotes, dashes\n",
        "    \"\"\"\n",
        "#     text = text.lower()\n",
        "    text = re.sub('[\\[\\(].*?[\\)\\]]', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[\\“\\–]', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjNtcYgDQ7FV"
      },
      "source": [
        "# clean text\n",
        "df['transcript'] = pd.DataFrame(df['transcript'].apply(lambda x: clean_text(x)))\n",
        "science_df['transcript'] = pd.DataFrame(science_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "technology_df['transcript'] = pd.DataFrame(technology_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "math_df['transcript'] = pd.DataFrame(math_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "computers_df['transcript'] = pd.DataFrame(computers_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "engineering_df['transcript'] = pd.DataFrame(engineering_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "ML_df['transcript'] = pd.DataFrame(ML_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "software_df['transcript'] = pd.DataFrame(software_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "statistics_df['transcript'] = pd.DataFrame(statistics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "cognitive_science_df['transcript'] = pd.DataFrame(cognitive_science_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "science_and_art_df['transcript'] = pd.DataFrame(science_and_art_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "physics_df['transcript'] = pd.DataFrame(physics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "quantum_physics_df['transcript'] = pd.DataFrame(quantum_physics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "code_df['transcript'] = pd.DataFrame(code_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "programming_df['transcript'] = pd.DataFrame(programming_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "chemistry_df['transcript'] = pd.DataFrame(chemistry_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "data_df['transcript'] = pd.DataFrame(data_df['transcript'].apply(lambda x: clean_text(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR7dHLSn7jCA"
      },
      "source": [
        "dfs = [science_df, technology_df, math_df, computers_df, engineering_df, ML_df,\n",
        "       software_df, statistics_df, cognitive_science_df, science_and_art_df, physics_df, \n",
        "       quantum_physics_df, code_df, programming_df, chemistry_df, data_df]\n",
        "       \n",
        "comb_df = pd.concat(dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "U5lcp0q8AeLl",
        "outputId": "48d5a8a1-89f9-4b0a-fd22-72edfeb0209a"
      },
      "source": [
        "comb_df.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>talk_id</th>\n",
              "      <th>topics</th>\n",
              "      <th>transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
              "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>['TED Prize', 'collaboration', 'disease', 'ebo...</td>\n",
              "      <td>I'm the luckiest guy in the world. I got to se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>['cognitive science', 'culture', 'evolution', ...</td>\n",
              "      <td>I'd like to talk today about the two biggest s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>98</td>\n",
              "      <td>['astronomy', 'biology', 'cognitive science', ...</td>\n",
              "      <td>My title: \"Queerer than we can suppose: the st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>47</td>\n",
              "      <td>['climate change', 'cosmos', 'culture', 'envir...</td>\n",
              "      <td>We've been told to go out on a limb and say so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>20554</td>\n",
              "      <td>['communication', 'compassion', 'identity', 'd...</td>\n",
              "      <td>We live in a world where the collection of dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1844</th>\n",
              "      <td>39331</td>\n",
              "      <td>['social change', 'social media', 'democracy',...</td>\n",
              "      <td>So, on the day after the Brexit vote, in June ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>46535</td>\n",
              "      <td>['inequality', 'crime', 'justice system', 'cul...</td>\n",
              "      <td>When people meet me for the first time on my j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1846</th>\n",
              "      <td>53582</td>\n",
              "      <td>['news', 'Internet', 'social media', 'global i...</td>\n",
              "      <td>So, on April  of , the Associated Press put ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>58018</td>\n",
              "      <td>['work', 'gender equality', 'business', 'gende...</td>\n",
              "      <td>A few years ago, I had a corporate feminist dr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1848 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      talk_id  ...                                         transcript\n",
              "0           1  ...  Thank you so much, Chris. And it's truly a gre...\n",
              "1          58  ...  I'm the luckiest guy in the world. I got to se...\n",
              "2          16  ...  I'd like to talk today about the two biggest s...\n",
              "3          98  ...  My title: \"Queerer than we can suppose: the st...\n",
              "4          47  ...  We've been told to go out on a limb and say so...\n",
              "...       ...  ...                                                ...\n",
              "1843    20554  ...  We live in a world where the collection of dat...\n",
              "1844    39331  ...  So, on the day after the Brexit vote, in June ...\n",
              "1845    46535  ...  When people meet me for the first time on my j...\n",
              "1846    53582  ...  So, on April  of , the Associated Press put ou...\n",
              "1847    58018  ...  A few years ago, I had a corporate feminist dr...\n",
              "\n",
              "[1848 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D0fmoeTflon"
      },
      "source": [
        "#comb_df\n",
        "scripts = comb_df[\"transcript\"].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxMT2AWNr1Rc"
      },
      "source": [
        "new_scrips = []\n",
        "length_sents = []\n",
        "\n",
        "for i in range(len(scripts)):\n",
        "  script = re.sub('\\.', ' <eos>', scripts[i])\n",
        "  script = re.sub('\\!', ' <eos>', script)\n",
        "  script = re.sub('\\?', ' <eos>', script)\n",
        "  script = re.sub('\\,', '', script)\n",
        "  script = re.sub('\\;', '', script)\n",
        "  script = re.sub('\\:', '', script)\n",
        "  script = re.sub('\\—', '', script)\n",
        "  script = re.sub('\\-', '', script)\n",
        "  script = re.sub('\\\"', '', script)\n",
        "  script = re.sub('  ', ' ', script)\n",
        "  script = re.sub('  ', ' ', script)\n",
        "  script = re.sub('<eos> <eos> <eos>', '', script)\n",
        "  test_scrips.append(script)\n",
        "\n",
        "  sen_lengths = []\n",
        "  split_sents = comb_sents5.split('<eos>')\n",
        "\n",
        "  for sent in split_sents:\n",
        "    words = sent.split(' ')\n",
        "    length = len(words)\n",
        "    sen_lengths.append(length)\n",
        "\n",
        "  length_sents.append(sen_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrw-WANnbqTw"
      },
      "source": [
        "list_idx = []\n",
        "for i in range(len(length_sents)):\n",
        "    max_num = max(length_sents[i])\n",
        "    if max_num >= 80:\n",
        "        list_idx.append(i)\n",
        "\n",
        "for i in range(len(list_idx)):\n",
        "    new_scripts.pop(list_idx[i] - i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLqAb6RAhPjq"
      },
      "source": [
        "def load_vocab(text):\n",
        "    word_to_ix = {}\n",
        "    for sent in text:\n",
        "        for word in sent.split():\n",
        "            word = word.lower()\n",
        "            word_to_ix.setdefault(word, len(word_to_ix))\n",
        "    return word_to_ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuI-2fRdbtoA"
      },
      "source": [
        "train_data1 = []\n",
        "for idx in range(len(new_scripts)):\n",
        "    split_sents = new_scripts[idx].split('<eos>') \n",
        "    sentence = []\n",
        "    for sent in split_sents:\n",
        "        words = sent.split()\n",
        "        words = [words[i].lower() for i in range(len(words))]\n",
        "        words.append('<eos> ')\n",
        "        sentence.append(' '.join(words))\n",
        "    data = ''.join(sentence)\n",
        "    train_data1.append(data)\n",
        "\n",
        "word2idx = load_vocab(train_data1)\n",
        "\n",
        "train_data = []\n",
        "for sent in train_data1:\n",
        "    words = []\n",
        "    for word in sent.split():\n",
        "        words.append(word2idx[word])\n",
        "    train_data.append(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IztR4CHUSeId"
      },
      "source": [
        "word_counts = {}\n",
        "\n",
        "for i in range(len(train_data1)):\n",
        "  for j in range(len(train_data1[i].split())):\n",
        "    if train_data1[i].split()[j] in word_counts:\n",
        "      word_counts[train_data1[i].split()[j]] += 1\n",
        "    else:\n",
        "      word_counts[train_data1[i].split()[j]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBLPDFk2UHLq"
      },
      "source": [
        "word_counts_list = []\n",
        "for key in word_counts:\n",
        "  word_counts_list.append((key, word_counts[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBE3-88Kd5ab"
      },
      "source": [
        "ordered_list = sorted(word_counts_list, key = lambda word: word[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAWyOqvsfE32"
      },
      "source": [
        "# creates sequences of a certain length\n",
        "\n",
        "seq_length = 32\n",
        "\n",
        "train_words = []\n",
        "train_idx = []\n",
        "for i in range(len(train_data1)):\n",
        "  for j in range(0, len(train_data1[i].split()) - seq_length, seq_length):\n",
        "    seq_wds = ' '.join(train_data1[i].split()[j : j + seq_length])\n",
        "    train_words.append(seq_wds)\n",
        "    seq_idx = train_data[i][j : j + seq_length]\n",
        "    train_idx.append(seq_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNZcUyi7gTxS",
        "outputId": "08b7ba09-dbd9-4352-88c9-036f223ff578"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# 80 percent train, 10 percent validation, 10 percent test split\n",
        "\n",
        "end1 = round(len(train_idx)*.8) # to get 80% for training\n",
        "end2 = round(len(train_idx)*.9) # to get 10% for validation and test\n",
        "print(end1)\n",
        "print(end2)\n",
        "\n",
        "random.shuffle(train_idx) # shuffle the data to get a better distribution\n",
        "\n",
        "train = torch.Tensor(train_idx[0:end1])\n",
        "val_data = torch.Tensor(train_idx[end1:end2])\n",
        "test_data = torch.Tensor(train_idx[end2:])\n",
        "\n",
        "train_data = train.long()\n",
        "val_data = val_data.long()\n",
        "test_data = test_data.long()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127377\n",
            "143299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpYI7-0sfUat"
      },
      "source": [
        "class twoLayer_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layers):\n",
        "        super().__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers)\n",
        "        self.lin_layer = nn.Linear(hidden_size, vocab_size)\n",
        "        # if want to make bi directional\n",
        "        #self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers, bidirectional=True)\n",
        "        #self.lin_layer = nn.Linear(hidden_size*2, vocab_size)\n",
        "\n",
        "    def forward(self, word_seq, h_init, c_init):\n",
        "        g_seq = self.emb_layer(word_seq)  \n",
        "        h_seq, (h_last, c_last) = self.rec_layer(g_seq, (h_init, c_init))\n",
        "        score_seq = self.lin_layer(h_seq)\n",
        "        return score_seq, (h_last, c_last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK9nfpHBfN7Y"
      },
      "source": [
        "def evaluate(data):\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "    with torch.no_grad():\n",
        "        h = torch.zeros(layers, bs, hidden_size)\n",
        "        c = torch.zeros(layers, bs, hidden_size)\n",
        "        h = h.to(device)\n",
        "        c = c.to(device)\n",
        "        for count in range(0, len(data) - bs, bs):\n",
        "            minibatch_data = data[count:count + bs]\n",
        "            minibatch_label = data[count+1:count + bs + 1]\n",
        "            minibatch_data = minibatch_data.to(device)\n",
        "            minibatch_label = minibatch_label.to(device)\n",
        "            scores, (h, c) = net(minibatch_data, h, c)\n",
        "            minibatch_label = minibatch_label.view(bs * seq_length) \n",
        "            scores = scores.view(bs * seq_length, vocab_size)\n",
        "            loss = criterion(scores, minibatch_label)    \n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "            num_batches += 1  \n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtMBtSLcfFPO"
      },
      "source": [
        "# setup NN\n",
        "hidden_size = 200\n",
        "vocab_size = len(word2idx)\n",
        "layers = 2\n",
        "num_epoch = 10\n",
        "bs = 32\n",
        "my_lr = 4.0\n",
        "\n",
        "net = twoLayer_LSTM(vocab_size, hidden_size, layers)\n",
        "net.emb_layer.weight.data.uniform_(-0.1, 0.1)\n",
        "net.lin_layer.weight = net.emb_layer.weight\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_size = len(train_data)\n",
        "optimizer = optim.SGD(net.parameters(), lr=my_lr, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ47Sb7dCCMo"
      },
      "source": [
        "def normalize_gradient(net):\n",
        "    grad_norm_sq = 0\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "    grad_norm = math.sqrt(grad_norm_sq)\n",
        "    if grad_norm < 1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "             p.grad.data.div_(grad_norm)\n",
        "    return grad_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "Ovkg4U3CYUMG",
        "outputId": "78a50d68-189a-4f70-e29c-919b644cd9da"
      },
      "source": [
        "# training\n",
        "start = time.time()\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    if epoch > 0 and epoch < 4 or epoch > 5: \n",
        "        my_lr *= 0.5\n",
        "        optimizer = optim.SGD(net.parameters(), lr=my_lr, momentum=0.9)\n",
        "            \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "       \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(layers, bs, hidden_size)\n",
        "    c = torch.zeros(layers, bs, hidden_size)\n",
        "    # send it to the gpu    \n",
        "    h = h.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    for count in range(0, train_size - bs, bs):    \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data = train_data[count : count + bs]\n",
        "        minibatch_label = train_data[count + 1 : count + bs + 1]        \n",
        "                \n",
        "        # send them to the gpu\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h = h.detach()\n",
        "        c = c.detach()\n",
        "        h = h.requires_grad_()\n",
        "        c = c.requires_grad_()\n",
        "        # forward the minibatch through the net \n",
        "        scores, (h, c) = net(minibatch_data, h, c)\n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores = scores.view(bs * seq_length, vocab_size)  \n",
        "        minibatch_label = minibatch_label.view(bs * seq_length)       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update the running loss  \n",
        "        #running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "                          \n",
        "    #total_loss = running_loss/num_batches\n",
        "    elapsed = time.time() - start\n",
        "    print('\\nepoch =', epoch, '\\t time =', elapsed,'\\t lr =', my_lr, '\\t training loss =', loss.item()) # compute error on the test set at end of each epoch\n",
        "    val_loss = evaluate(val_data) # eval on the validation set\n",
        "    train_loss_list.append(loss.item())\n",
        "    val_loss_list.append(val_loss)\n",
        "    test_loss = evaluate(test_data) # eval on the test set\n",
        "    test_loss_list.append(test_loss)\n",
        "    print('val loss =', val_loss)\n",
        "    print('test loss =', test_loss)\n",
        "\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5866c0b70619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mnormalize_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-6949f3605c0d>\u001b[0m in \u001b[0;36mnormalize_gradient\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m              \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L0mQzB6kjPO"
      },
      "source": [
        "x = range(0, num_epoch,1)\n",
        "\n",
        "plt.plot(x, train_loss_list, '.-', label='Train Loss')\n",
        "plt.plot(x, val_loss_list, '.-', label='Validation Loss')\n",
        "plt.plot(x, test_loss_list, '.-', label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mojCAN2KweOj"
      },
      "source": [
        "param = net.state_dict()\n",
        "torch.save(param, 'trained_parameters_LSTM.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCCdbrCtHyrn"
      },
      "source": [
        "idx2word = {y:x for x, y in word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am1geAsSGi9L"
      },
      "source": [
        "def show_most_likely_words(prob):\n",
        "    num_word_display = 15\n",
        "    p = prob.view(-1)\n",
        "    p, word_idx = torch.topk(p, num_word_display)\n",
        "    for i, idx in enumerate(word_idx):\n",
        "        percentage = p[i].item() * 100\n",
        "        word = idx2word[idx.item()]\n",
        "        print(\"{:.1f}%\\t\".format(percentage), word) \n",
        "\n",
        "def text2tensor(text):\n",
        "    text = text.lower()\n",
        "    list_of_words = text.split()\n",
        "    list_of_int = [word2idx[w] for w in list_of_words]\n",
        "    x = torch.LongTensor(list_of_int)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Tu0gW1CKBU"
      },
      "source": [
        "sentence = \"machine learning is\"\n",
        "\n",
        "h = torch.zeros(layers, bs, hidden_size)\n",
        "c = torch.zeros(layers, bs, hidden_size)\n",
        "h = h.to(device)\n",
        "c = c.to(device)\n",
        "\n",
        "data = text2tensor(sentence)\n",
        "seq_len = len(data)\n",
        "data = data.view(seq_len, -1)\n",
        "empty = torch.zeros(seq_len, bs - 1).type(torch.LongTensor)\n",
        "data = torch.cat((data, empty), dim=1)\n",
        "data = data.to(device)\n",
        "scores, (h, c) = net(data, h, c)\n",
        "scores = scores[seq_len - 1, 0, :]\n",
        "p = F.softmax(scores.view(1, vocab_size), dim=1)\n",
        "print(sentence, '... \\n')\n",
        "show_most_likely_words(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbsvQ9jiGaWH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}