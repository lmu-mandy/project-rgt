{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of baseline_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UgKbXYvqPjZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from ast import literal_eval\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fFEVH58Og0T"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/lmu-mandy/project-rgt/bob-branch/ted_talks_en.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df = df.loc[:, ['talk_id', 'topics', 'transcript']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRHtVN-7fgNT"
      },
      "source": [
        "sep_topics = df.topics.unique()\n",
        "topics = []\n",
        "\n",
        "for topic in sep_topics:\n",
        "    for i in topic.split(\",\"):\n",
        "        topics.append(i.split(\"'\")[1])\n",
        "print(topics[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC2uwR-X-uUW"
      },
      "source": [
        "unique_topics = [] \n",
        "      \n",
        "# traverse for all elements \n",
        "for topic in topics: \n",
        "    # check if exists in unique_list or not \n",
        "    if topic not in unique_topics: \n",
        "            unique_topics.append(topic) \n",
        "print(unique_topics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBxKsFIOg-R"
      },
      "source": [
        "def find_topic(topic):\n",
        "    \"\"\"Returns a list of booleans for talks that contain a topic by index.\n",
        "    \n",
        "    :param topic: Topics or related topics of a talk\n",
        "    \"\"\"\n",
        "    has_topic = []\n",
        "    for t_list in df['topics']:\n",
        "        if topic.lower() in literal_eval(t_list):\n",
        "            has_topic.append(1)\n",
        "        else:\n",
        "            has_topic.append(0)\n",
        "    return has_topic"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p3dPW0WOhE6"
      },
      "source": [
        "# add columns for selected topics\n",
        "df['is_science'] = find_topic('science')\n",
        "df['is_technology'] = find_topic('technology')\n",
        "df['is_math'] = find_topic('math')\n",
        "df['is_computers'] = find_topic('computers')\n",
        "df['is_engineering'] = find_topic('engineering')\n",
        "df['is_ML'] = find_topic('machine learning')\n",
        "df['is_software'] = find_topic('software')\n",
        "df['is_statistics'] = find_topic('statistics')\n",
        "df['is_cognitive_science'] = find_topic('cognitive science')\n",
        "df['is_science_and_art'] = find_topic('science and art')\n",
        "df['is_physics'] = find_topic('physics')\n",
        "df['is_quantum_physics'] = find_topic('quantum physics')\n",
        "df['is_code'] = find_topic('code')\n",
        "df['is_programming'] = find_topic('programming')\n",
        "df['is_chemistry'] = find_topic('chemistry')\n",
        "df['is_data'] = find_topic('data')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxP796sxQWy5"
      },
      "source": [
        "# filter DataFrame to only include talks about sex, religion, and politics\n",
        "df = df.loc[(df['is_science']==1) | (df['is_technology']==1) | \n",
        "            (df['is_math']==1) | (df['is_computers']==1) |\n",
        "            (df['is_engineering']==1) | (df['is_ML']==1) | \n",
        "            (df['is_software'] == 1) | (df['is_statistics'] == 1) | \n",
        "            (df['is_cognitive_science'] == 1) | (df['is_science_and_art'] == 1) | \n",
        "            (df['is_physics'] == 1) | (df['is_quantum_physics'] == 1) | \n",
        "            (df['is_code'] == 1) | (df['is_programming'] == 1) | \n",
        "            (df['is_chemistry'] == 1) | df['is_data'] == 1, : ].reset_index(drop=True)\n",
        "\n",
        "# create new DataFrames for each topic (for later use)\n",
        "science_df = df.loc[(df['is_science']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "technology_df = df.loc[(df['is_technology']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "math_df = df.loc[(df['is_math']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "computers_df = df.loc[(df['is_computers']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "engineering_df = df.loc[(df['is_engineering']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "ML_df = df.loc[(df['is_ML']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "software_df = df.loc[(df['is_software']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "statistics_df = df.loc[(df['is_statistics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "cognitive_science_df = df.loc[(df['is_cognitive_science']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "science_and_art_df = df.loc[(df['is_science_and_art']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "physics_df = df.loc[(df['is_physics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "quantum_physics_df = df.loc[(df['is_quantum_physics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "code_df = df.loc[(df['is_code']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "programming_df = df.loc[(df['is_programming']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "chemistry_df = df.loc[(df['is_chemistry']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "data_df = df.loc[(df['is_data']==1), 'talk_id':'transcript'].reset_index(drop=True)\n",
        "\n",
        "print('Science', science_df.shape)\n",
        "print('Technology', technology_df.shape)\n",
        "print('Math', math_df.shape)\n",
        "print('Computers', computers_df.shape)\n",
        "print('Engineering', engineering_df.shape)\n",
        "print('Machine Learning', ML_df.shape)\n",
        "print('Software', software_df.shape)\n",
        "print('Statistics', statistics_df.shape)\n",
        "print('Cognitive Science', cognitive_science_df.shape)\n",
        "print('Science and Art', science_and_art_df.shape)\n",
        "print('Physics', physics_df.shape)\n",
        "print('Quantum Physics', quantum_physics_df.shape)\n",
        "print('Code', code_df.shape)\n",
        "print('Programming', programming_df.shape)\n",
        "print('Chemistry', chemistry_df.shape)\n",
        "print('Data', data_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6f0IkfAQW5P"
      },
      "source": [
        "def combine_transcripts(transcript_list):\n",
        "    \"\"\"Input a list of transcripts and return them as a corpus.\n",
        "    :param list_of_text: Transcript list\"\"\"\n",
        "    corpus = ' '.join(transcript_list)\n",
        "    return corpus\n",
        "\n",
        "def transcripts_to_dict(df, topic_list):\n",
        "    \"\"\"Returns a dictionary of transcripts for each topic.\n",
        "    \n",
        "    :param df: DataFrame\n",
        "    :param topic_list: List of topics\n",
        "    \"\"\"\n",
        "    ted_dict = {}\n",
        "    for topic in topic_list:\n",
        "        # filter DataFrame to specific series and convert it to a list\n",
        "        filter_string = 'is_' + str(topic)\n",
        "        text_list = df.loc[(df[filter_string] == 1), 'transcript'].to_list()\n",
        "\n",
        "        # call combine_transcripts function to return combined text\n",
        "        combined_text = combine_transcripts(text_list)\n",
        "\n",
        "        # add combined text to dict\n",
        "        ted_dict[topic] = combined_text\n",
        "    return ted_dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uKNRdNeQXGR"
      },
      "source": [
        "# create dictionary from the DataFrame\n",
        "transcript_dict = transcripts_to_dict(df, ['science', 'technology', 'math', 'computers', 'engineering', 'ML', \n",
        "                                           'software', 'statistics', 'cognitive_science', 'science_and_art', 'physics', \n",
        "                                           'quantum_physics', 'code', 'programming', 'chemistry', 'data'])\n",
        "\n",
        "# construct DataFrame from dictionary\n",
        "df = pd.DataFrame.from_dict(transcript_dict, orient='index')\n",
        "df.rename({0: 'transcript'}, axis=1, inplace=True)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esArp9P9Qsjh"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Returns clean text.\n",
        "    Removes:\n",
        "        *text in square brackets & parenthesis\n",
        "        *punctuation\n",
        "        *words containing numbers\n",
        "        *double-quotes, dashes\n",
        "    \"\"\"\n",
        "#     text = text.lower()\n",
        "    text = re.sub('[\\[\\(].*?[\\)\\]]', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[\\“\\–]', '', text)\n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjNtcYgDQ7FV"
      },
      "source": [
        "# clean text\n",
        "df['transcript'] = pd.DataFrame(df['transcript'].apply(lambda x: clean_text(x)))\n",
        "science_df['transcript'] = pd.DataFrame(science_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "technology_df['transcript'] = pd.DataFrame(technology_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "math_df['transcript'] = pd.DataFrame(math_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "computers_df['transcript'] = pd.DataFrame(computers_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "engineering_df['transcript'] = pd.DataFrame(engineering_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "ML_df['transcript'] = pd.DataFrame(ML_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "software_df['transcript'] = pd.DataFrame(software_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "statistics_df['transcript'] = pd.DataFrame(statistics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "cognitive_science_df['transcript'] = pd.DataFrame(cognitive_science_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "science_and_art_df['transcript'] = pd.DataFrame(science_and_art_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "physics_df['transcript'] = pd.DataFrame(physics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "quantum_physics_df['transcript'] = pd.DataFrame(quantum_physics_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "code_df['transcript'] = pd.DataFrame(code_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "programming_df['transcript'] = pd.DataFrame(programming_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "chemistry_df['transcript'] = pd.DataFrame(chemistry_df['transcript'].apply(lambda x: clean_text(x)))\n",
        "data_df['transcript'] = pd.DataFrame(data_df['transcript'].apply(lambda x: clean_text(x)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR7dHLSn7jCA"
      },
      "source": [
        "dfs = [science_df, technology_df, math_df, computers_df, engineering_df, ML_df,\n",
        "       software_df, statistics_df, cognitive_science_df, science_and_art_df, physics_df, \n",
        "       quantum_physics_df, code_df, programming_df, chemistry_df, data_df]\n",
        "       \n",
        "comb_df = pd.concat(dfs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5lcp0q8AeLl"
      },
      "source": [
        "comb_df.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D0fmoeTflon"
      },
      "source": [
        "#comb_df\n",
        "scripts = comb_df[\"transcript\"].to_numpy()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxMT2AWNr1Rc"
      },
      "source": [
        "new_scripts = []\n",
        "length_sents = []\n",
        "\n",
        "for i in range(len(scripts)):\n",
        "  script = re.sub('\\.', ' <eos>', scripts[i])\n",
        "  script = re.sub('\\!', ' <eos>', script)\n",
        "  script = re.sub('\\?', ' <eos>', script)\n",
        "  script = re.sub('\\,', '', script)\n",
        "  script = re.sub('\\;', '', script)\n",
        "  script = re.sub('\\:', '', script)\n",
        "  script = re.sub('\\—', '', script)\n",
        "  script = re.sub('\\-', '', script)\n",
        "  script = re.sub('\\\"', '', script)\n",
        "  script = re.sub('  ', ' ', script)\n",
        "  script = re.sub('  ', ' ', script)\n",
        "  script = re.sub('<eos> <eos> <eos>', '', script)\n",
        "  new_scripts.append(script)\n",
        "\n",
        "  sen_lengths = []\n",
        "  split_sents = script.split('<eos>')\n",
        "\n",
        "  for sent in split_sents:\n",
        "    words = sent.split(' ')\n",
        "    length = len(words)\n",
        "    sen_lengths.append(length)\n",
        "\n",
        "  length_sents.append(sen_lengths)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrw-WANnbqTw"
      },
      "source": [
        "list_idx = []\n",
        "for i in range(len(length_sents)):\n",
        "    max_num = max(length_sents[i])\n",
        "    if max_num >= 80:\n",
        "        list_idx.append(i)\n",
        "\n",
        "for i in range(len(list_idx)):\n",
        "    new_scripts.pop(list_idx[i] - i)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuI-2fRdbtoA"
      },
      "source": [
        "train_data1 = []\n",
        "for idx in range(len(new_scripts)):\n",
        "    split_sents = new_scripts[idx].split('<eos>') \n",
        "    sentence = []\n",
        "    for sent in split_sents:\n",
        "        words = sent.split()\n",
        "        words = [words[i].lower() for i in range(len(words))]\n",
        "        words.append('<eos> ')\n",
        "        sentence.append(' '.join(words))\n",
        "    data = ''.join(sentence)\n",
        "    train_data1.append(data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IztR4CHUSeId"
      },
      "source": [
        "word_counts = {}\n",
        "\n",
        "for i in range(len(train_data1)):\n",
        "  for j in range(len(train_data1[i].split())):\n",
        "    if train_data1[i].split()[j] in word_counts:\n",
        "      word_counts[train_data1[i].split()[j]] += 1\n",
        "    else:\n",
        "      word_counts[train_data1[i].split()[j]] = 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBLPDFk2UHLq"
      },
      "source": [
        "word_counts_list = []\n",
        "for key in word_counts:\n",
        "  word_counts_list.append((key, word_counts[key]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBE3-88Kd5ab"
      },
      "source": [
        "ordered_list = sorted(word_counts_list, key = lambda word: word[1], reverse=True)\n",
        "ordered_list = ordered_list[0:10000]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UuRqb-Nxczy"
      },
      "source": [
        "word2idx = {}\n",
        "for i in range(len(ordered_list)):\n",
        "  word2idx[ordered_list[i][0]] = i\n",
        "print(len(word2idx))\n",
        "word2idx['<unk>'] = len(word2idx)\n",
        "print(len(word2idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_KrR38szjIb"
      },
      "source": [
        "train_data_new = []\n",
        "train_data1_new = []\n",
        "\n",
        "for sent in train_data1:\n",
        "    words = []\n",
        "    idx = []\n",
        "    for word in sent.split():\n",
        "      if word in word2idx:\n",
        "        words.append(word)\n",
        "        idx.append(word2idx[word])\n",
        "      else:\n",
        "        words.append('<unk>')\n",
        "        idx.append(len(word2idx))\n",
        "    train_data_new.append(idx)\n",
        "    train_data1_new.append(' '.join(words))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAWyOqvsfE32"
      },
      "source": [
        "# creates sequences of a certain length\n",
        "\n",
        "seq_length = 32\n",
        "\n",
        "train_words = []\n",
        "train_words_new = []\n",
        "train_idx_new = []\n",
        "\n",
        "for i in range(len(train_data1)):\n",
        "  for j in range(0, len(train_data1[i].split()) - seq_length, seq_length):\n",
        "    seq_wds = ' '.join(train_data1[i].split()[j : j + seq_length])\n",
        "    train_words.append(seq_wds)\n",
        "\n",
        "    seq_wds_new = ' '.join(train_data1_new[i].split()[j : j + seq_length])\n",
        "    train_words_new.append(seq_wds_new)\n",
        "\n",
        "    seq_idx_new = train_data_new[i][j : j + seq_length]\n",
        "    train_idx_new.append(seq_idx_new)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjBLduqz_VrP"
      },
      "source": [
        "print(train_words[1])\n",
        "print(train_words_new[1])\n",
        "print(train_idx_new[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNZcUyi7gTxS"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# 80 percent train, 10 percent validation, 10 percent test split\n",
        "\n",
        "end1 = round(len(train_idx_new)*.9) # to get 90% for training\n",
        "end2 = round(len(train_idx_new)*.95) # to get 5% for validation and test\n",
        "print(end1)\n",
        "print(end2)\n",
        "\n",
        "random.shuffle(train_idx_new) # shuffle the data to get a better distribution\n",
        "\n",
        "train = torch.Tensor(train_idx_new[0:end1])\n",
        "val_data = torch.Tensor(train_idx_new[end1:end2])\n",
        "test_data = torch.Tensor(train_idx_new[end2:])\n",
        "\n",
        "train_data = train.long()\n",
        "val_data = val_data.long()\n",
        "test_data = test_data.long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpYI7-0sfUat"
      },
      "source": [
        "class twoLayer_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layers):\n",
        "        super().__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers)\n",
        "        self.lin_layer = nn.Linear(hidden_size, vocab_size)\n",
        "        # if want to make bi directional\n",
        "        #self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers, bidirectional=True)\n",
        "        #self.lin_layer = nn.Linear(hidden_size*2, vocab_size)\n",
        "\n",
        "    def forward(self, word_seq, h_init, c_init):\n",
        "        g_seq = self.emb_layer(word_seq)  \n",
        "        h_seq, (h_last, c_last) = self.rec_layer(g_seq, (h_init, c_init))\n",
        "        score_seq = self.lin_layer(h_seq)\n",
        "        return score_seq, (h_last, c_last)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK9nfpHBfN7Y"
      },
      "source": [
        "def evaluate(data):\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "    with torch.no_grad():\n",
        "        h = torch.zeros(layers, bs, hidden_size)\n",
        "        c = torch.zeros(layers, bs, hidden_size)\n",
        "        h = h.to(device)\n",
        "        c = c.to(device)\n",
        "        for count in range(0, len(data) - bs, bs):\n",
        "            minibatch_data = data[count:count + bs]\n",
        "            minibatch_label = data[count+1:count + bs + 1]\n",
        "            minibatch_data = minibatch_data.to(device)\n",
        "            minibatch_label = minibatch_label.to(device)\n",
        "            scores, (h, c) = net(minibatch_data, h, c)\n",
        "            minibatch_label = minibatch_label.view(bs * seq_length) \n",
        "            scores = scores.view(bs * seq_length, vocab_size)\n",
        "            loss = criterion(scores, minibatch_label)    \n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "            num_batches += 1  \n",
        "    return loss.item()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtMBtSLcfFPO"
      },
      "source": [
        "# setup NN\n",
        "hidden_size = 100\n",
        "vocab_size = len(word2idx)+1\n",
        "layers = 2\n",
        "num_epoch = 10\n",
        "bs = 32\n",
        "my_lr = 4.0\n",
        "\n",
        "net = twoLayer_LSTM(vocab_size, hidden_size, layers)\n",
        "net.emb_layer.weight.data.uniform_(-0.1, 0.1)\n",
        "net.lin_layer.weight = net.emb_layer.weight\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_size = len(train_data)\n",
        "optimizer = optim.SGD(net.parameters(), lr=my_lr, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ47Sb7dCCMo"
      },
      "source": [
        "def normalize_gradient(net):\n",
        "    grad_norm_sq = 0\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "    grad_norm = math.sqrt(grad_norm_sq)\n",
        "    if grad_norm < 1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "             p.grad.data.div_(grad_norm)\n",
        "    return grad_norm"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovkg4U3CYUMG"
      },
      "source": [
        "# training\n",
        "start = time.time()\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    if epoch > 0 and epoch < 4 or epoch > 5: \n",
        "        my_lr *= 0.5\n",
        "        optimizer = optim.SGD(net.parameters(), lr=my_lr, momentum=0.9)\n",
        "            \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "       \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(layers, bs, hidden_size)\n",
        "    c = torch.zeros(layers, bs, hidden_size)\n",
        "    # send it to the gpu    \n",
        "    h = h.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    for count in range(0, train_size - bs, bs):    \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data = train_data[count : count + bs]\n",
        "        minibatch_label = train_data[count + 1 : count + bs + 1]        \n",
        "                \n",
        "        # send them to the gpu\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h = h.detach()\n",
        "        c = c.detach()\n",
        "        h = h.requires_grad_()\n",
        "        c = c.requires_grad_()\n",
        "        # forward the minibatch through the net \n",
        "        scores, (h, c) = net(minibatch_data, h, c)\n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores = scores.view(bs * seq_length, vocab_size)  \n",
        "        minibatch_label = minibatch_label.view(bs * seq_length)       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update the running loss  \n",
        "        #running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "                          \n",
        "    #total_loss = running_loss/num_batches\n",
        "    elapsed = time.time() - start\n",
        "    print('\\nepoch =', epoch, '\\t time =', elapsed,'\\t lr =', my_lr, '\\t training loss =', loss.item()) # compute error on the test set at end of each epoch\n",
        "    val_loss = evaluate(val_data) # eval on the validation set\n",
        "    train_loss_list.append(loss.item())\n",
        "    val_loss_list.append(val_loss)\n",
        "    test_loss = evaluate(test_data) # eval on the test set\n",
        "    test_loss_list.append(test_loss)\n",
        "    print('val loss =', val_loss)\n",
        "    print('test loss =', test_loss)\n",
        "\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L0mQzB6kjPO"
      },
      "source": [
        "x = range(0, num_epoch,1)\n",
        "\n",
        "plt.plot(x, train_loss_list, '.-', label='Train Loss')\n",
        "plt.plot(x, val_loss_list, '.-', label='Validation Loss')\n",
        "plt.plot(x, test_loss_list, '.-', label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mojCAN2KweOj"
      },
      "source": [
        "param = net.state_dict()\n",
        "torch.save(param, 'trained_parameters_LSTM.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCCdbrCtHyrn"
      },
      "source": [
        "idx2word = {y:x for x, y in word2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am1geAsSGi9L"
      },
      "source": [
        "def show_most_likely_words(prob):\n",
        "    num_word_display = 15\n",
        "    p = prob.view(-1)\n",
        "    p, word_idx = torch.topk(p, num_word_display)\n",
        "    for i, idx in enumerate(word_idx):\n",
        "        percentage = p[i].item() * 100\n",
        "        word = idx2word[idx.item()]\n",
        "        print(\"{:.1f}%\\t\".format(percentage), word) \n",
        "\n",
        "def text2tensor(text):\n",
        "    text = text.lower()\n",
        "    list_of_words = text.split()\n",
        "    list_of_int = [word2idx[w] for w in list_of_words]\n",
        "    x = torch.LongTensor(list_of_int)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Tu0gW1CKBU"
      },
      "source": [
        "sentence = \"machine learning is\"\n",
        "\n",
        "h = torch.zeros(layers, bs, hidden_size)\n",
        "c = torch.zeros(layers, bs, hidden_size)\n",
        "h = h.to(device)\n",
        "c = c.to(device)\n",
        "\n",
        "data = text2tensor(sentence)\n",
        "seq_len = len(data)\n",
        "data = data.view(seq_len, -1)\n",
        "empty = torch.zeros(seq_len, bs - 1).type(torch.LongTensor)\n",
        "data = torch.cat((data, empty), dim=1)\n",
        "data = data.to(device)\n",
        "scores, (h, c) = net(data, h, c)\n",
        "scores = scores[seq_len - 1, 0, :]\n",
        "p = F.softmax(scores.view(1, vocab_size), dim=1)\n",
        "print(sentence, '... \\n')\n",
        "show_most_likely_words(p)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}