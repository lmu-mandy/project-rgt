# -*- coding: utf-8 -*-
"""GPT-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pk8NBEieWeEqn8d7zsGJDYwzazShkEBm
"""

# only need to run this line once
!pip install simpletransformers

prompts = [
    "Hello everyone, today I'll be discussing"
    # enter your own prompts here
]

# packages needed to run this code
import pandas as pd
import numpy as np
import logging
import torch
from simpletransformers.language_modeling import LanguageModelingModel
from simpletransformers.language_generation import LanguageGenerationModel

# import the data
url = 'https://github.com/lmu-mandy/project-rgt/blob/main/preprocessed_data.csv.zip?raw=true'
df = pd.read_csv(url, compression='zip', header=0, sep=',', quotechar='"', index_col=0)
print(df.shape)
df.head()

# check if GPU is available
if torch.cuda.is_available():    
    device = torch.device('cuda')
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device('cpu')

# convert transcripts column to a list
transcripts = df['transcript'].tolist()

# split data into training and testing sets
index = int(len(transcripts) * 0.8)

# 80% training
with open("train.txt", "w") as f:
    for transcript in transcripts[:-index]:
        f.writelines(transcript + "\n")

# 20% testing
with open("test.txt", "w") as f:
    for transcript in transcripts[-index:]:
        f.writelines(transcript + "\n")

# language generation using a pre-trained GPT-2 model
logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

model = LanguageGenerationModel("gpt2", "gpt2", args={"max_length": 300})

# print out the prompt and generated text
for prompt in prompts:
    generated = model.generate(prompt, verbose=False)
    generated = '.'.join(generated[0].split('.')[:-1]) + '.'
    print('Prompt:', prompt)
    print('')
    print('Generated text:', generated)
    print('')

# evaluate a non-fine tuned GPT-2 model on the test data
baseline_model = LanguageModelingModel('gpt2', 'gpt2', args={"mlm": False})
baseline_model.eval_model('test.txt')

# specify parameters for training
train_args = {
    "reprocess_input_data": True,
    "overwrite_output_dir": True,
    "train_batch_size": 20,
    "num_train_epochs": 8,
    "mlm": False, 
    "learning_rate": 4e-7,
    "adam_epsilon": 1e-9
}

# fine-tune the pre-trained GPT-2 model using the training data and the parameters from above
model = LanguageModelingModel('gpt2', 'gpt2', args=train_args)
model.train_model("train.txt", eval_file="test.txt")
model.eval_model("test.txt")

# generate text using the trained model
text_generator = LanguageGenerationModel("gpt2", "./outputs", args={"max_length": 300})

# print out the prompt and generated text
for prompt in prompts:
    generated = text_generator.generate(prompt, verbose=False)
    generated = '.'.join(generated[0].split('.')[:-1]) + '.'
    print('Prompt:', prompt)
    print('')
    print('Generated text:', generated)
    print('')