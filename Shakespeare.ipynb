{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnsBDceRhwrT",
        "outputId": "19556b98-b044-4af0-ac88-62e3abc92a27"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFw9-WG7iDCq"
      },
      "source": [
        "text = re.sub('\\.', ' <eos>', text)\n",
        "text = re.sub('\\!', ' <eos>', text)\n",
        "text = re.sub('\\?', ' <eos>', text)\n",
        "text = re.sub('\\,', '', text)\n",
        "text = re.sub('\\:', ' :', text)\n",
        "text = re.sub('\\;', '', text)\n",
        "text = re.sub('\\--', ' <eos>', text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lj7cAztjB_D"
      },
      "source": [
        "text = ' '.join(text.split())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytmkcpmXnPSw"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If4abeOdnpA6"
      },
      "source": [
        "text_lower = []\n",
        "for i in text.split():\n",
        "  i = i.lower()\n",
        "  text_lower.append(i)\n",
        "  \n",
        "text_lower = ' '.join(text_lower)\n",
        "text_lower"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y73nXZMEmo4a"
      },
      "source": [
        "word_counts = {}\n",
        "\n",
        "for i in range(len(text_lower.split())):\n",
        "  if text_lower.split()[i] in word_counts:\n",
        "    word_counts[text_lower.split()[i]] += 1\n",
        "  else:\n",
        "    word_counts[text_lower.split()[i]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA0LnWWrohPZ"
      },
      "source": [
        "word_counts_list = []\n",
        "for key in word_counts:\n",
        "  word_counts_list.append((key, word_counts[key]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5PnqyhIok-U"
      },
      "source": [
        "ordered_list = sorted(word_counts_list, key = lambda word: word[1], reverse=True)\n",
        "print(len(ordered_list))\n",
        "ordered_list = ordered_list[0:5000]\n",
        "ordered_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzaPpHiJolA_",
        "outputId": "2aef30e7-293a-4c11-ef11-94e994cd024b"
      },
      "source": [
        "word2idx = {}\n",
        "for i in range(len(ordered_list)):\n",
        "  word2idx[ordered_list[i][0]] = i\n",
        "print(len(word2idx))\n",
        "word2idx['<unk>'] = len(word2idx)\n",
        "print(len(word2idx))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uru38r4Uo74A"
      },
      "source": [
        "text_words = []\n",
        "text_idx = []\n",
        "\n",
        "for word in text_lower.split():\n",
        "    words = []\n",
        "    idx = []\n",
        "    if word in word2idx:\n",
        "      text_words.append(word)\n",
        "      text_idx.append(word2idx[word])\n",
        "    else:\n",
        "      words.append('<unk>')\n",
        "      idx.append(len(word2idx)-1)\n",
        "\n",
        "text_words = ' '.join(text_words)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msCbU-3iuwNy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "fbf64ccf-51d3-4443-c9f9-bc4fe6e2e89e"
      },
      "source": [
        "#creates sequences of a certain length\n",
        "\n",
        "seq_length = 32\n",
        "\n",
        "text_words_seq = []\n",
        "text_idx_seq = []\n",
        "\n",
        "for i in range(len(text_words.split()) - seq_length):\n",
        "  seq_wds = ' '.join(text_words.split()[i : i + seq_length])\n",
        "  text_words_seq.append(seq_wds)\n",
        "\n",
        "  seq_idxs = text_idx[i : i + seq_length]\n",
        "  text_idx_seq.append(seq_idxs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-25d2f15a15ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mseq_wds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mtext_words_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_wds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQTwXbYBhJWW",
        "outputId": "10d69608-cf55-4e57-ecda-adff1756eedc"
      },
      "source": [
        "#text_idx_seq = [t.numpy() for t in text_idx_seq]\n",
        "#data = torch.tensor(text_idx_seq)\n",
        "\n",
        "# 80 percent train, 10 percent validation, 10 percent test split\n",
        "\n",
        "end1 = round(len(text_words_seq)*.8) # to get 90% for training\n",
        "end2 = round(len(text_words_seq)*.9) # to get 5% for validation and test\n",
        "print(end1)\n",
        "print(end2)\n",
        "\n",
        "train_data = torch.Tensor(data[0:end1])\n",
        "val_data = torch.Tensor(data[end1:end2])\n",
        "test_data = torch.Tensor(data[end2:])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127386\n",
            "143310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAIy4VCkygt2"
      },
      "source": [
        "class twoLayer_LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, layers):\n",
        "        super().__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers)\n",
        "        self.lin_layer = nn.Linear(hidden_size, vocab_size)\n",
        "        # if want to make bi directional\n",
        "        #self.rec_layer = nn.LSTM(hidden_size, hidden_size, num_layers=layers, bidirectional=True)\n",
        "        #self.lin_layer = nn.Linear(hidden_size*2, vocab_size)\n",
        "\n",
        "    def forward(self, word_seq, h_init, c_init):\n",
        "        g_seq = self.emb_layer(word_seq)  \n",
        "        h_seq, (h_last, c_last) = self.rec_layer(g_seq, (h_init, c_init))\n",
        "        score_seq = self.lin_layer(h_seq)\n",
        "        return score_seq, (h_last, c_last)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEXgFadZykiv"
      },
      "source": [
        "def evaluate(data):\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "    with torch.no_grad():\n",
        "        h = torch.zeros(layers, bs, hidden_size)\n",
        "        c = torch.zeros(layers, bs, hidden_size)\n",
        "        h = h.to(device)\n",
        "        c = c.to(device)\n",
        "        for count in range(0, len(data) - bs, bs):\n",
        "            minibatch_data = data.long()[count:count + bs]\n",
        "            minibatch_label = data.long()[count+1:count + bs + 1]\n",
        "            minibatch_data = minibatch_data.to(device)\n",
        "            minibatch_label = minibatch_label.to(device)\n",
        "            scores, (h, c) = net(minibatch_data, h, c)\n",
        "            minibatch_label = minibatch_label.view(bs * seq_length) \n",
        "            scores = scores.view(bs * seq_length, vocab_size)\n",
        "            loss = criterion(scores, minibatch_label)    \n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "            num_batches += 1  \n",
        "    return loss.item()\n",
        "\n",
        "def normalize_gradient(net):\n",
        "    grad_norm_sq = 0\n",
        "    for p in net.parameters():\n",
        "        grad_norm_sq += p.grad.data.norm()**2\n",
        "    grad_norm = math.sqrt(grad_norm_sq)\n",
        "    if grad_norm < 1e-4:\n",
        "        net.zero_grad()\n",
        "        print('grad norm close to zero')\n",
        "    else:    \n",
        "        for p in net.parameters():\n",
        "             p.grad.data.div_(grad_norm)\n",
        "    return grad_norm"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iWhgMqlytzZ"
      },
      "source": [
        "# setup NN\n",
        "hidden_size = 100\n",
        "vocab_size = len(word2idx)+1\n",
        "layers = 2\n",
        "num_epoch = 8\n",
        "bs = 32\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "net = twoLayer_LSTM(vocab_size, hidden_size, layers)\n",
        "net.emb_layer.weight.data.uniform_(-0.1, 0.1)\n",
        "net.lin_layer.weight = net.emb_layer.weight\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_size = len(train_data)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw6hIHGfyuv0",
        "outputId": "36337b94-872b-47e4-98c9-6fc58e9656fe"
      },
      "source": [
        "# training with SGD\n",
        "start = time.time()\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    #if epoch > 0:\n",
        "    my_lr = 1.5 * math.exp(-0.5 * epoch)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=my_lr, momentum=0.9)\n",
        "            \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "       \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(layers, bs, hidden_size)\n",
        "    c = torch.zeros(layers, bs, hidden_size)\n",
        "    # send it to the gpu    \n",
        "    h = h.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    for count in range(0, train_size - bs, bs):    \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data = train_data.long()[count : count + bs]\n",
        "        minibatch_label = train_data.long()[count + 1 : count + bs + 1]\n",
        "                \n",
        "        # send them to the gpu\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h = h.detach()\n",
        "        c = c.detach()\n",
        "        h = h.requires_grad_()\n",
        "        c = c.requires_grad_()\n",
        "        # forward the minibatch through the net \n",
        "        scores, (h, c) = net(minibatch_data, h, c)\n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores = scores.view(bs * seq_length, vocab_size)  \n",
        "        minibatch_label = minibatch_label.view(bs * seq_length)       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update the running loss  \n",
        "        #running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "                          \n",
        "    #total_loss = running_loss/num_batches\n",
        "    elapsed = time.time() - start\n",
        "    print('\\nepoch =', epoch, '\\t time = {0:.1f}'.format(elapsed),'\\t lr = {0:.3f}'.format(my_lr), '\\t training loss = {0:.3f}'.format(loss.item())) # compute error on the test set at end of each epoch\n",
        "    val_loss = evaluate(val_data) # eval on the validation set\n",
        "    train_loss_list.append(loss.item())\n",
        "    val_loss_list.append(val_loss)\n",
        "    test_loss = evaluate(test_data) # eval on the test set\n",
        "    test_loss_list.append(test_loss)\n",
        "    print('val loss = {0:.3f}'.format(val_loss))\n",
        "    print('test loss = {0:.3f}'.format(test_loss))\n",
        "\n",
        "print(\" \")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch = 0 \t time = 84.9 \t lr = 1.500 \t training loss = 6.019\n",
            "val loss = 6.670\n",
            "test loss = 6.551\n",
            "\n",
            "epoch = 1 \t time = 172.5 \t lr = 0.910 \t training loss = 5.510\n",
            "val loss = 6.292\n",
            "test loss = 5.964\n",
            "\n",
            "epoch = 2 \t time = 259.9 \t lr = 0.552 \t training loss = 5.163\n",
            "val loss = 6.089\n",
            "test loss = 5.833\n",
            "\n",
            "epoch = 3 \t time = 346.3 \t lr = 0.335 \t training loss = 4.980\n",
            "val loss = 5.969\n",
            "test loss = 5.608\n",
            "\n",
            "epoch = 4 \t time = 431.9 \t lr = 0.203 \t training loss = 4.876\n",
            "val loss = 5.787\n",
            "test loss = 5.368\n",
            "\n",
            "epoch = 5 \t time = 518.2 \t lr = 0.123 \t training loss = 4.801\n",
            "val loss = 5.668\n",
            "test loss = 5.304\n",
            "\n",
            "epoch = 6 \t time = 605.2 \t lr = 0.075 \t training loss = 4.735\n",
            "val loss = 5.605\n",
            "test loss = 5.278\n",
            "\n",
            "epoch = 7 \t time = 692.9 \t lr = 0.045 \t training loss = 4.684\n",
            "val loss = 5.544\n",
            "test loss = 5.282\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBE_wJ1b91ya",
        "outputId": "206ddcd6-a10d-4fd5-d0f8-246feebd2cab"
      },
      "source": [
        "# training with Adagrad\n",
        "start = time.time()\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "test_loss_list = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    #if epoch > 0:\n",
        "    my_lr = 0.1 * math.exp(-0.5 * epoch)\n",
        "    optimizer = optim.Adagrad(net.parameters(), lr=my_lr)\n",
        "            \n",
        "    # set the running quantities to zero at the beginning of the epoch\n",
        "    running_loss = 0\n",
        "    num_batches = 0    \n",
        "       \n",
        "    # set the initial h to be the zero vector\n",
        "    h = torch.zeros(layers, bs, hidden_size)\n",
        "    c = torch.zeros(layers, bs, hidden_size)\n",
        "    # send it to the gpu    \n",
        "    h = h.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    for count in range(0, train_size - bs, bs):    \n",
        "        # Set the gradients to zeros\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # create a minibatch\n",
        "        minibatch_data = train_data.long()[count : count + bs]\n",
        "        minibatch_label = train_data.long()[count + 1 : count + bs + 1]        \n",
        "                \n",
        "        # send them to the gpu\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        \n",
        "        # Detach to prevent from backpropagating all the way to the beginning\n",
        "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
        "        h = h.detach()\n",
        "        c = c.detach()\n",
        "        h = h.requires_grad_()\n",
        "        c = c.requires_grad_()\n",
        "        # forward the minibatch through the net \n",
        "        scores, (h, c) = net(minibatch_data, h, c)\n",
        "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
        "        scores = scores.view(bs * seq_length, vocab_size)  \n",
        "        minibatch_label = minibatch_label.view(bs * seq_length)       \n",
        "        \n",
        "        # Compute the average of the losses of the data points in this huge batch\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "        \n",
        "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
        "        normalize_gradient(net)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update the running loss  \n",
        "        #running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "                          \n",
        "    #total_loss = running_loss/num_batches\n",
        "    elapsed = time.time() - start\n",
        "    print('\\nepoch =', epoch, '\\t time = {0:.1f}'.format(elapsed),'\\t lr = {0:.3f}'.format(my_lr), '\\t training loss = {0:.3f}'.format(loss.item())) # compute error on the test set at end of each epoch\n",
        "    val_loss = evaluate(val_data) # eval on the validation set\n",
        "    train_loss_list.append(loss.item())\n",
        "    val_loss_list.append(val_loss)\n",
        "    test_loss = evaluate(test_data) # eval on the test set\n",
        "    test_loss_list.append(test_loss)\n",
        "    print('val loss = {0:.3f}'.format(val_loss))\n",
        "    print('test loss = {0:.3f}'.format(test_loss))\n",
        "\n",
        "print(\" \")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch = 0 \t time = 84.2 \t lr = 0.100 \t training loss = 4.541\n",
            "val loss = 5.423\n",
            "test loss = 5.368\n",
            "\n",
            "epoch = 1 \t time = 169.9 \t lr = 0.061 \t training loss = 4.569\n",
            "val loss = 5.524\n",
            "test loss = 5.411\n",
            "\n",
            "epoch = 2 \t time = 254.4 \t lr = 0.037 \t training loss = 4.453\n",
            "val loss = 5.427\n",
            "test loss = 5.368\n",
            "\n",
            "epoch = 3 \t time = 338.4 \t lr = 0.022 \t training loss = 4.485\n",
            "val loss = 5.482\n",
            "test loss = 5.462\n",
            "\n",
            "epoch = 4 \t time = 422.5 \t lr = 0.014 \t training loss = 4.469\n",
            "val loss = 5.476\n",
            "test loss = 5.495\n",
            "\n",
            "epoch = 5 \t time = 506.2 \t lr = 0.008 \t training loss = 4.473\n",
            "val loss = 5.533\n",
            "test loss = 5.527\n",
            "\n",
            "epoch = 6 \t time = 590.4 \t lr = 0.005 \t training loss = 4.464\n",
            "val loss = 5.539\n",
            "test loss = 5.503\n",
            "\n",
            "epoch = 7 \t time = 675.8 \t lr = 0.003 \t training loss = 4.480\n",
            "val loss = 5.525\n",
            "test loss = 5.469\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "D1XRnGHiyuzB",
        "outputId": "ebefdb0e-6be4-45e3-d296-bbc3e5099ab4"
      },
      "source": [
        "x = range(0, num_epoch,1)\n",
        "\n",
        "plt.plot(x, train_loss_list, '.-', label='Train Loss')\n",
        "plt.plot(x, val_loss_list, '.-', label='Val Loss')\n",
        "plt.plot(x, test_loss_list, '.-', label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ero_vAW_zdwc"
      },
      "source": [
        "idx2word = {y:x for x, y in word2idx.items()}"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHrChCwtzged"
      },
      "source": [
        "def show_most_likely_words(prob):\n",
        "    num_word_display = 15\n",
        "    p = prob.view(-1)\n",
        "    p, word_idx = torch.topk(p, num_word_display)\n",
        "    for i, idx in enumerate(word_idx):\n",
        "        percentage = p[i].item() * 100\n",
        "        word = idx2word[idx.item()]\n",
        "        print(\"{:.1f}%\\t\".format(percentage), word) \n",
        "\n",
        "def text2tensor(text):\n",
        "    text = text.lower()\n",
        "    list_of_words = text.split()\n",
        "    list_of_idx = []\n",
        "    for w in list_of_words:\n",
        "      if w in word2idx:\n",
        "        idx = word2idx[w]\n",
        "        list_of_idx.append(idx)\n",
        "      else:\n",
        "        list_of_idx.append(len(word2idx)-1)\n",
        "    x = torch.LongTensor(list_of_idx)\n",
        "    return x"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVSZHX7mzmAl",
        "outputId": "d7bdc877-de73-4b30-95ef-9fd17a6ad344"
      },
      "source": [
        "sentence = \"thou shalt more pikes <eos> menenius : i have not to the people <eos> lady : no no <eos>\"\n",
        "\n",
        "h = torch.zeros(layers, bs, hidden_size)\n",
        "c = torch.zeros(layers, bs, hidden_size)\n",
        "h = h.to(device)\n",
        "c = c.to(device)\n",
        "\n",
        "data = text2tensor(sentence)\n",
        "seq_len = len(data)\n",
        "data = data.view(seq_len, -1)\n",
        "empty = torch.zeros(seq_len, bs - 1).type(torch.LongTensor)\n",
        "data = torch.cat((data, empty), dim=1)\n",
        "data = data.to(device)\n",
        "scores, (h, c) = net(data, h, c)\n",
        "scores = scores[seq_len - 1, 0, :]\n",
        "p = F.softmax(scores.view(1, vocab_size), dim=1)\n",
        "print(sentence, '... \\n')\n",
        "show_most_likely_words(p)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thou shalt more pikes <eos> menenius : i have not to the people <eos> lady : no no <eos> ... \n",
            "\n",
            "20.9%\t gloucester\n",
            "4.6%\t queen\n",
            "3.6%\t why\n",
            "3.4%\t king\n",
            "2.5%\t clarence\n",
            "2.0%\t hastings\n",
            "1.8%\t i\n",
            "1.7%\t what\n",
            "1.7%\t lady\n",
            "1.5%\t edward\n",
            "1.2%\t messenger\n",
            "1.2%\t rivers\n",
            "1.1%\t richard\n",
            "1.0%\t third\n",
            "1.0%\t and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDHlWDDIzq-A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}